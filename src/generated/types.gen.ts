// This file is auto-generated by @hey-api/openapi-ts

/**
 * Base input for AI Toolkit training across all ecosystems
 */
export type AiToolkitTrainingInput = TrainingInput & {
  engine: 'ai-toolkit';
} & {
  ecosystem: string;
  /**
   * Number of training epochs. An epoch is one complete pass through the training dataset.
   * Maximum of 20 epochs can be specified.
   * Note: ai-toolkit uses steps internally, calculated based on epochs, image count, and internal parameters.
   */
  epochs?: number;
  /**
   * Specify the maximum resolution of training images. If the training images exceed the resolution specified here, they will be scaled down to this resolution
   */
  resolution?: number | null;
  /**
   * Sets the learning rate for the model. This is the learning rate when performing additional learning on each attention block (and other blocks depending on the setting).
   */
  lr?: number;
  /**
   * Sets the learning rate for the text encoder. Only used when TrainTextEncoder is true. For models with multiple text encoders, this applies to all of them.
   */
  textEncoderLr?: number | null;
  /**
   * Whether to train the text encoder(s) alongside the model. Enabling this can improve prompt understanding but increases training time and memory usage.
   */
  trainTextEncoder?: boolean | null;
  /**
   * You can change the learning rate in the middle of learning. A scheduler is a setting for how to change the learning rate.
   */
  lrScheduler?: 'constant' | 'constant_with_warmup' | 'cosine' | 'linear' | 'step';
  /**
   * The optimizer determines how to update the neural net weights during training.
   * Various methods have been proposed for smart learning, but the most commonly used in LoRA learning is "adamw8bit".
   */
  optimizerType?:
    | 'adam'
    | 'adamw'
    | 'adamw8bit'
    | 'adam8bit'
    | 'lion'
    | 'lion8bit'
    | 'adafactor'
    | 'adagrad'
    | 'prodigy'
    | 'prodigy8bit';
  /**
   * The larger the Dim setting, the more learning information can be stored, but the possibility of learning unnecessary information other than the learning target increases. A larger Dim also increases LoRA file size.
   */
  networkDim?: number | null;
  /**
   * The smaller the Network alpha value, the larger the stored LoRA neural net weights.
   * For example, with an Alpha of 16 and a Dim of 32, the strength of the weight used is 16/32 = 0.5,
   * meaning that the learning rate is only half as powerful as the Learning Rate setting.
   *
   * If Alpha and Dim are the same number, the strength used will be 1 and will have no effect on the learning rate.
   */
  networkAlpha?: number | null;
  /**
   * Adds noise to training images. 0 adds no noise at all. A value of 1 adds strong noise.
   */
  noiseOffset?: number | null;
  /**
   * If this option is turned on, the image will be horizontally flipped randomly. It can learn left and right angles, which is useful when you want to learn symmetrical people and objects.
   */
  flipAugmentation?: boolean;
  /**
   * Randomly changes the order of your tags during training. The intent of shuffling is to improve learning. If you are using captions (sentences), this option has no meaning.
   */
  shuffleTokens?: boolean;
  /**
   * If your training images have tags, you can randomly shuffle them.
   * However, if you have words that you want to keep at the beginning, you can use this option to specify "Keep the first 0 words at the beginning".
   * This option does nothing if the Shuffle Tokens option is off.
   */
  keepTokens?: number;
} & {
  engine: 'ai-toolkit';
};

export type AgeClassificationInput = {
  /**
   * An optional model to use for age classification. If not provided, the default model will determined by the worker
   */
  model?: string | null;
  /**
   * The URL of the media to classify. This can either be a URL to an image or a video or a ZIP containing multiple images
   */
  mediaUrl: string;
};

export type AgeClassificationOutput = {
  labels: {
    [key: string]: Array<AgeClassifierLabel>;
  };
  hasMinor: boolean;
};

/**
 * Age classification
 */
export type AgeClassificationStep = WorkflowStep & {
  $type: 'ageClassification';
} & {
  input: AgeClassificationInput;
  output?: AgeClassificationOutput;
} & {
  $type: 'ageClassification';
};

/**
 * Age classification
 */
export type AgeClassificationStepTemplate = WorkflowStepTemplate & {
  $type: 'ageClassification';
} & {
  input: AgeClassificationInput;
} & {
  $type: 'ageClassification';
};

export type AgeClassifierLabel = {
  age: string;
  isMinor: boolean;
  boundingBox: Array<number>;
};

export const AnimalPoseBboxDetector = {
  YOLOX_L_TORCHSCRIPT_PT: 'yolox_l.torchscript.pt',
  YOLOX_L_ONNX: 'yolox_l.onnx',
  YOLO_NAS_L_FP16_ONNX: 'yolo_nas_l_fp16.onnx',
  YOLO_NAS_M_FP16_ONNX: 'yolo_nas_m_fp16.onnx',
  YOLO_NAS_S_FP16_ONNX: 'yolo_nas_s_fp16.onnx',
} as const;

export type AnimalPoseBboxDetector =
  (typeof AnimalPoseBboxDetector)[keyof typeof AnimalPoseBboxDetector];

export const AnimalPoseEstimator = {
  RTMPOSE_M_AP10K_256_BS5_TORCHSCRIPT_PT: 'rtmpose-m_ap10k_256_bs5.torchscript.pt',
  RTMPOSE_M_AP10K_256_ONNX: 'rtmpose-m_ap10k_256.onnx',
} as const;

export type AnimalPoseEstimator = (typeof AnimalPoseEstimator)[keyof typeof AnimalPoseEstimator];

export const AnylineMergeWith = {
  LINEART_STANDARD: 'lineart_standard',
  LINEART_REALISTIC: 'lineart_realistic',
  LINEART_ANIME: 'lineart_anime',
  MANGA_LINE: 'manga_line',
} as const;

export type AnylineMergeWith = (typeof AnylineMergeWith)[keyof typeof AnylineMergeWith];

export type BatchOcrSafetyClassificationInput = {
  mediaUrls: Array<string>;
};

export type BatchOcrSafetyClassificationOutput = {
  results: Array<BatchOcrSafetyClassificationResult>;
};

export type BatchOcrSafetyClassificationResult = {
  mediaUrl: string;
  classification: string;
  text?: string | null;
};

/**
 * Represents a blob that gets produced as part of a specific job
 */
export type Blob = {
  type: string;
  /**
   * Gets the id of the blob that contains this image.
   */
  id: string;
  /**
   * Gets a value indicating whether the blob is available.
   */
  available: boolean;
  /**
   * Gets a url that can be used to preview the blob.
   */
  url?: string | null;
  /**
   * Get when the url is set to expire
   */
  urlExpiresAt?: string | null;
  /**
   * Get the id of the job that is associated with this blob.
   */
  jobId?: string | null;
  nsfwLevel?: NsfwLevel;
  /**
   * Get an optional reason for why the blob was blocked. This is only set if the blob was blocked.
   */
  blockedReason?: string | null;
};

export const BuzzClientAccount = {
  YELLOW: 'yellow',
  BLUE: 'blue',
  GREEN: 'green',
  FAKE_RED: 'fakeRed',
} as const;

export type BuzzClientAccount = (typeof BuzzClientAccount)[keyof typeof BuzzClientAccount];

/**
 * AI Toolkit training for Chroma models
 */
export type ChromaAiToolkitTrainingInput = AiToolkitTrainingInput & {} & {
  ecosystem: 'chroma';
};

export const CoarseMode = {
  DISABLE: 'disable',
  ENABLE: 'enable',
} as const;

export type CoarseMode = (typeof CoarseMode)[keyof typeof CoarseMode];

export type ComfyInput = {
  /**
   * Get the comfy workflow that needs to be executed
   */
  comfyWorkflow: {
    [key: string]: ComfyNode;
  };
  /**
   * The number of jobs to start with this workflow.
   */
  quantity?: number;
  /**
   * External metadata that will be stored with the image
   */
  imageMetadata?: string | null;
  /**
   * Opt-into using the spine controller exclusively
   */
  useSpineComfy?: boolean | null;
};

export type ComfyNode = {
  classType: string;
  meta?: {
    [key: string]: string;
  } | null;
  isChanged?: string | null;
  inputs: {
    [key: string]: string | number | boolean | [number, number];
  };
};

export type ComfyOutput = {
  /**
   * Get a list of blobs that got generated by this comfy workflow step.
   */
  blobs?: Array<Blob>;
};

/**
 * Comfy workflows
 */
export type ComfyStep = WorkflowStep & {
  $type: 'comfy';
} & {
  input: ComfyInput;
  output?: ComfyOutput;
} & {
  $type: 'comfy';
};

/**
 * Comfy workflows
 */
export type ComfyStepTemplate = WorkflowStepTemplate & {
  $type: 'comfy';
} & {
  input: ComfyInput;
} & {
  $type: 'comfy';
};

export const ContainerFormat = {
  MP4: 'mp4',
  WEB_M: 'webM',
} as const;

export type ContainerFormat = (typeof ContainerFormat)[keyof typeof ContainerFormat];

export type CursedArrayOfTelemetryCursorAndWorkflow = {
  next: string;
  items: Array<Workflow>;
};

export const DensePoseColormap = {
  'VIRIDIS (_MAGIC_ANIMATE)': 'Viridis (MagicAnimate)',
  'PARULA (_CIVIT_AI)': 'Parula (CivitAI)',
} as const;

export type DensePoseColormap = (typeof DensePoseColormap)[keyof typeof DensePoseColormap];

export const DensePoseModel = {
  DENSEPOSE_R50_FPN_DL_TORCHSCRIPT: 'densepose_r50_fpn_dl.torchscript',
  DENSEPOSE_R101_FPN_DL_TORCHSCRIPT: 'densepose_r101_fpn_dl.torchscript',
} as const;

export type DensePoseModel = (typeof DensePoseModel)[keyof typeof DensePoseModel];

export const DepthAnythingCheckpoint = {
  DEPTH_ANYTHING_VITL14_PTH: 'depth_anything_vitl14.pth',
  DEPTH_ANYTHING_VITB14_PTH: 'depth_anything_vitb14.pth',
  DEPTH_ANYTHING_VITS14_PTH: 'depth_anything_vits14.pth',
} as const;

export type DepthAnythingCheckpoint =
  (typeof DepthAnythingCheckpoint)[keyof typeof DepthAnythingCheckpoint];

export const DepthAnythingV2Checkpoint = {
  DEPTH_ANYTHING_V2_VITG_PTH: 'depth_anything_v2_vitg.pth',
  DEPTH_ANYTHING_V2_VITL_PTH: 'depth_anything_v2_vitl.pth',
  DEPTH_ANYTHING_V2_VITB_PTH: 'depth_anything_v2_vitb.pth',
  DEPTH_ANYTHING_V2_VITS_PTH: 'depth_anything_v2_vits.pth',
} as const;

export type DepthAnythingV2Checkpoint =
  (typeof DepthAnythingV2Checkpoint)[keyof typeof DepthAnythingV2Checkpoint];

export const DwPoseBboxDetector = {
  YOLOX_L_ONNX: 'yolox_l.onnx',
  YOLOX_L_TORCHSCRIPT_PT: 'yolox_l.torchscript.pt',
  YOLO_NAS_L_FP16_ONNX: 'yolo_nas_l_fp16.onnx',
  YOLO_NAS_M_FP16_ONNX: 'yolo_nas_m_fp16.onnx',
  YOLO_NAS_S_FP16_ONNX: 'yolo_nas_s_fp16.onnx',
} as const;

export type DwPoseBboxDetector = (typeof DwPoseBboxDetector)[keyof typeof DwPoseBboxDetector];

export const DwPoseEstimator = {
  DW_LL_UCOCO_384_BS5_TORCHSCRIPT_PT: 'dw-ll_ucoco_384_bs5.torchscript.pt',
  DW_LL_UCOCO_384_ONNX: 'dw-ll_ucoco_384.onnx',
  DW_LL_UCOCO_ONNX: 'dw-ll_ucoco.onnx',
} as const;

export type DwPoseEstimator = (typeof DwPoseEstimator)[keyof typeof DwPoseEstimator];

/**
 * Represents the input information needed for the Echo workflow step.
 */
export type EchoInput = {
  /**
   * The message to be returned in the output.
   */
  message: string;
};

/**
 * Represents the output information returned from the Echo workflow step.
 */
export type EchoOutput = {
  /**
   * The message to be returned.
   */
  message: string;
};

/**
 * Echo
 */
export type EchoStep = WorkflowStep & {
  $type: 'echo';
} & {
  input: EchoInput;
  output?: EchoOutput;
} & {
  $type: 'echo';
};

/**
 * Echo
 */
export type EchoStepTemplate = WorkflowStepTemplate & {
  $type: 'echo';
} & {
  input: EchoInput;
} & {
  $type: 'echo';
};

/**
 * An epock result.
 */
export type EpochResult = {
  epochNumber?: number;
  /**
   * Get the name of the generated epoch assets
   */
  blobName: string;
  /**
   * Get the total size in bytes of the asset
   */
  blobSize?: number | null;
  /**
   * Get a list of the names of the blobs that represent sample images
   */
  sampleImages?: Array<string>;
  /**
   * A presigned url that points to the epoch file
   */
  blobUrl: string;
};

export const FileFormat = {
  UNKNOWN: 'unknown',
  SAFE_TENSOR: 'safeTensor',
  PICKLE_TENSOR: 'pickleTensor',
  DIFFUSERS: 'diffusers',
  CORE_ML: 'coreML',
  ONNX: 'onnx',
  TAR: 'tar',
} as const;

export type FileFormat = (typeof FileFormat)[keyof typeof FileFormat];

/**
 * AI Toolkit training for Flux.1 models
 */
export type Flux1AiToolkitTrainingInput = AiToolkitTrainingInput & {
  modelVariant: string;
} & {
  ecosystem: 'flux1';
};

/**
 * AI Toolkit training for Flux.1 Dev models
 */
export type Flux1DevAiToolkitTrainingInput = Flux1AiToolkitTrainingInput & {} & {
  modelVariant: 'dev';
};

export type Flux1KontextDevImageGenInput = Flux1KontextImageGenInput & {
  readonly model: string;
} & {
  model: 'dev';
};

export type Flux1KontextImageGenInput = ImageGenInput & {
  engine: 'flux1-kontext';
} & {
  readonly model: string;
  prompt: string;
  images?: Array<string>;
  aspectRatio?: '21:9' | '16:9' | '4:3' | '3:2' | '1:1' | '2:3' | '3:4' | '9:16' | '9:21';
  outputFormat?: 'jpeg' | 'png';
  guidanceScale?: number;
  quantity?: number;
  seed?: number | null;
} & {
  engine: 'flux1-kontext';
};

export type Flux1KontextMaxImageGenInput = Flux1KontextImageGenInput & {
  readonly model: string;
} & {
  model: 'max';
};

export type Flux1KontextProImageGenInput = Flux1KontextImageGenInput & {
  readonly model: string;
} & {
  model: 'pro';
};

/**
 * AI Toolkit training for Flux.1 Schnell models
 */
export type Flux1SchnellAiToolkitTrainingInput = Flux1AiToolkitTrainingInput & {} & {
  modelVariant: 'schnell';
};

export type Flux2DevCreateImageInput = Flux2DevImageGenInput & {
  readonly operation: string;
} & {
  operation: 'createImage';
};

export type Flux2DevEditImageInput = Flux2DevImageGenInput & {
  images?: Array<string>;
  readonly operation: string;
} & {
  operation: 'editImage';
};

/**
 * Input for Flux 2 Dev image editing LoRA training via FAL.
 */
export type Flux2DevEditImageResourceTrainingInput = ImageResourceTrainingInput & {
  engine: 'flux2-dev-edit';
} & {
  /**
   * Number of training steps. Must be in increments of 100.
   */
  steps?: number;
  /**
   * Learning rate for training.
   */
  learningRate?: number;
  /**
   * Default caption to use if caption files are missing from training data.
   */
  defaultCaption?: string | null;
  /**
   * Number of reference images per training pair (1-4).
   * Affects cost calculation via reference multiplier.
   */
  referenceImageCount?: number;
} & {
  engine: 'flux2-dev-edit';
};

export type Flux2DevImageGenInput = Flux2ImageGenInput & {
  operation: string;
  guidanceScale?: number;
  numInferenceSteps?: number;
  loras?: Array<ImageGenInputLora>;
  readonly modelVariant?: string;
} & {
  model: 'dev';
};

/**
 * Input for Flux 2 Dev text-to-image LoRA training via FAL.
 */
export type Flux2DevImageResourceTrainingInput = ImageResourceTrainingInput & {
  engine: 'flux2-dev';
} & {
  /**
   * Number of training steps. Must be in increments of 100.
   */
  steps?: number;
  /**
   * Learning rate for training.
   */
  learningRate?: number;
  /**
   * Default caption to use if caption files are missing from training data.
   */
  defaultCaption?: string | null;
} & {
  engine: 'flux2-dev';
};

export type Flux2FlexCreateImageInput = Flux2FlexImageGenInput & {
  readonly operation: string;
} & {
  operation: 'createImage';
};

export type Flux2FlexEditImageInput = Flux2FlexImageGenInput & {
  images?: Array<string>;
  readonly operation: string;
} & {
  operation: 'editImage';
};

export type Flux2FlexImageGenInput = Flux2ImageGenInput & {
  operation: string;
  guidanceScale?: number;
  numInferenceSteps?: number;
  readonly modelVariant?: string;
} & {
  model: 'flex';
};

export type Flux2ImageGenInput = ImageGenInput & {
  engine: 'flux2';
} & {
  model: string;
  prompt: string;
  width?: number;
  height?: number;
  outputFormat?: 'jpeg' | 'png';
  seed?: number | null;
  quantity?: number;
  enablePromptExpansion?: boolean;
  /**
   * The model variant (dev, flex, pro)
   */
  readonly modelVariant?: string;
  /**
   * The operation type (createImage, editImage)
   */
  readonly operation?: string;
} & {
  engine: 'flux2';
};

export type Flux2ProCreateImageInput = Flux2ProImageGenInput & {
  readonly operation: string;
} & {
  operation: 'createImage';
};

export type Flux2ProEditImageInput = Flux2ProImageGenInput & {
  images?: Array<string>;
  readonly operation: string;
} & {
  operation: 'editImage';
};

export type Flux2ProImageGenInput = Flux2ImageGenInput & {
  operation: string;
  readonly modelVariant?: string;
} & {
  model: 'pro';
};

export type FluxDevFastImageResourceTrainingInput = ImageResourceTrainingInput & {
  engine: 'flux-dev-fast';
} & {} & {
  engine: 'flux-dev-fast';
};

export type Gemini25FlashCreateImageGenInput = Gemini25FlashImageGenInput & {} & {
  operation: 'createImage';
};

export type Gemini25FlashEditImageGenInput = Gemini25FlashImageGenInput & {
  images: Array<string>;
} & {
  operation: 'editImage';
};

export type Gemini25FlashImageGenInput = GeminiImageGenInput & {
  operation: string;
  quantity?: number;
} & {
  model: '2.5-flash';
};

export type GeminiImageGenInput = ImageGenInput & {
  engine: 'gemini';
} & {
  model: string;
  prompt: string;
} & {
  engine: 'gemini';
};

export type GoogleImageGenInput = ImageGenInput & {
  engine: 'google';
} & {
  model: string;
  prompt: string;
} & {
  engine: 'google';
};

export const HaiperVideoGenAspectRatio = {
  '16:9': '16:9',
  '4:3': '4:3',
  '1:1': '1:1',
  '9:16': '9:16',
  '3:4': '3:4',
} as const;

export type HaiperVideoGenAspectRatio =
  (typeof HaiperVideoGenAspectRatio)[keyof typeof HaiperVideoGenAspectRatio];

export const HaiperVideoGenCameraMovement = {
  NONE: 'none',
  PAN_RIGHT: 'panRight',
  PAN_LEFT: 'panLeft',
  TILT_UP: 'tiltUp',
  TILT_DOWN: 'tiltDown',
  ZOOM_IN: 'zoomIn',
  ZOOM_OUT: 'zoomOut',
} as const;

export type HaiperVideoGenCameraMovement =
  (typeof HaiperVideoGenCameraMovement)[keyof typeof HaiperVideoGenCameraMovement];

export type HaiperVideoGenInput = VideoGenInput & {
  engine: 'haiper';
} & {
  negativePrompt?: string | null;
  cameraMovement?: HaiperVideoGenCameraMovement;
  seed?: number;
  duration?: 2 | 4 | 8;
  aspectRatio?: HaiperVideoGenAspectRatio;
  model?: HaiperVideoGenModel;
  resolution?: 720 | 1080 | 2160;
  enablePromptEnhancer?: boolean;
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  sourceImage?: string | null;
} & {
  engine: 'haiper';
};

export const HaiperVideoGenModel = {
  V1_5: 'v1_5',
  V2: 'v2',
} as const;

export type HaiperVideoGenModel = (typeof HaiperVideoGenModel)[keyof typeof HaiperVideoGenModel];

export type HaiperVideoGenOutput = VideoGenOutput & {
  progress?: number | null;
  externalTOSViolation?: boolean | null;
  message?: string | null;
};

export const HumanoidImageMaskCategory = {
  DRESSES: 'dresses',
  UPPER_BODY: 'upperBody',
  LOWER_BODY: 'lowerBody',
} as const;

export type HumanoidImageMaskCategory =
  (typeof HumanoidImageMaskCategory)[keyof typeof HumanoidImageMaskCategory];

export type HumanoidImageMaskInput = {
  imageUrl: string;
  category: HumanoidImageMaskCategory;
};

export type HumanoidImageMaskOutput = {
  blob: Blob;
};

export type HunyuanVdeoGenInput = VideoGenInput & {
  engine: 'hunyuan';
} & {
  cfgScale?: number;
  frameRate?: number;
  duration?: number;
  seed?: number | null;
  steps?: number;
  width?: number;
  height?: number;
  loras?: Array<VideoGenInputLora>;
  model?: string | null;
} & {
  engine: 'hunyuan';
};

export type ImageBlob = Blob & {
  type: 'image';
} & {
  width?: number | null;
  height?: number | null;
} & {
  type: 'image';
};

export type ImageGenInput = {
  engine: string;
};

export type ImageGenInputLora = {
  air: string;
  strength?: number;
};

export type ImageGenOutput = {
  /**
   * A collection of output images.
   */
  images: Array<ImageBlob>;
  /**
   * An optional list of errors related to generation failures
   */
  errors?: Array<string> | null;
};

/**
 * Image Generation
 */
export type ImageGenStep = WorkflowStep & {
  $type: 'imageGen';
} & {
  input: ImageGenInput;
  output?: ImageGenOutput;
} & {
  $type: 'imageGen';
};

/**
 * Image Generation
 */
export type ImageGenStepTemplate = WorkflowStepTemplate & {
  $type: 'imageGen';
} & {
  input: ImageGenInput;
} & {
  $type: 'imageGen';
};

/**
 * Information for a controlnet provided for a text to image input.
 */
export type ImageJobControlNet = {
  preprocessor?: ImageTransformer;
  /**
   * A value representing the weight applied to the ControlNet.
   */
  weight?: number;
  /**
   * A value representing the start step selected for the ControlNet.
   */
  startStep?: number;
  /**
   * A value representing the end step selected for the ControlNet.
   */
  endStep?: number;
};

export type ImageJobNetworkParams = {
  /**
   * In case of Lora and LoCon, set the strength of the network
   */
  strength?: number | null;
  /**
   * In case of a TextualInversion, set the trigger word of the network
   */
  triggerWord?: string | null;
  /**
   * A legacy type set by the consumer
   */
  type?: string | null;
};

export const ImageResouceTrainingModerationStatus = {
  EVALUATING: 'evaluating',
  UNDER_REVIEW: 'underReview',
  APPROVED: 'approved',
  REJECTED: 'rejected',
} as const;

export type ImageResouceTrainingModerationStatus =
  (typeof ImageResouceTrainingModerationStatus)[keyof typeof ImageResouceTrainingModerationStatus];

/**
 * Input for an image resource training step.
 */
export type ImageResourceTrainingInput = {
  engine: string;
  /**
   * The primary model to train upon.
   */
  model: string;
  /**
   * A url referring data to use in training.
   */
  trainingData: string;
  /**
   * The number of images embedded in this training data. This is used to calculate the cost of training.
   */
  trainingDataImagesCount: number;
  /**
   * The desired lora name.
   */
  loraName?: string;
  /**
   * A selection of sample prompts.
   */
  samplePrompts?: Array<string>;
  /**
   * An optional negative prompt that will get applied when generating samples
   */
  negativePrompt?: string | null;
};

export type ImageResourceTrainingOutput = {
  moderationStatus: ImageResouceTrainingModerationStatus;
  /**
   * An array of epochs.
   */
  epochs: Array<EpochResult>;
  /**
   * The selected prompts for sample images
   */
  sampleImagesPrompts: Array<string>;
  /**
   * The selected images for sample images
   */
  sampleInputImages?: Array<string> | null;
  /**
   * Get wether the blobs are actually stored as assets
   * Assets are deprecated and require a different retrieval mechanism
   */
  storedAsAssets?: boolean | null;
  /**
   * Get an estimate in minutes on how long the work is expected to take
   */
  eta?: number | null;
};

/**
 * LORA Training
 */
export type ImageResourceTrainingStep = WorkflowStep & {
  $type: 'imageResourceTraining';
} & {
  input: ImageResourceTrainingInput;
  output?: ImageResourceTrainingOutput;
} & {
  $type: 'imageResourceTraining';
};

/**
 * LORA Training
 */
export type ImageResourceTrainingStepTemplate = WorkflowStepTemplate & {
  $type: 'imageResourceTraining';
} & {
  input: ImageResourceTrainingInput;
} & {
  $type: 'imageResourceTraining';
};

/**
 * Available image transformers.
 */
export const ImageTransformer = {
  CANNY: 'canny',
  DEPTH_ZOE: 'depthZoe',
  SOFTEDGE_PIDINET: 'softedgePidinet',
  REMBG: 'rembg',
} as const;

/**
 * Available image transformers.
 */
export type ImageTransformer = (typeof ImageTransformer)[keyof typeof ImageTransformer];

export type ImageUploadOutput = {
  blob: Blob;
};

/**
 * Image upload
 */
export type ImageUploadStep = WorkflowStep & {
  $type: 'imageUpload';
} & {
  /**
   * The workflow's input.
   */
  input: string;
  output?: ImageUploadOutput;
} & {
  $type: 'imageUpload';
};

/**
 * Image upload
 */
export type ImageUploadStepTemplate = WorkflowStepTemplate & {
  $type: 'imageUpload';
} & {
  /**
   * Input for the ImageUploadStep step.
   */
  input: string | null;
} & {
  $type: 'imageUpload';
};

export type ImageUpscalerInput = {
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  image: string;
  scaleFactor?: number;
};

export type ImageUpscalerOutput = {
  blob: ImageBlob;
};

export type ImageUpscalerStep = WorkflowStep & {
  $type: 'imageUpscaler';
} & {
  input: ImageUpscalerInput;
  output?: ImageUpscalerOutput;
} & {
  $type: 'imageUpscaler';
};

export type ImageUpscalerStepTemplate = WorkflowStepTemplate & {
  $type: 'imageUpscaler';
} & {
  input: ImageUpscalerInput;
} & {
  $type: 'imageUpscaler';
};

export type Imagen4ImageGenInput = GoogleImageGenInput & {
  prompt: string;
  negativePrompt?: string;
  aspectRatio?: '1:1' | '16:9' | '9:16' | '3:4' | '4:3';
  numImages?: number;
  seed?: number | null;
} & {
  model: 'imagen4';
};

/**
 * Available levels of job support.
 */
export const JobSupport = {
  UNSUPPORTED: 'unsupported',
  UNAVAILABLE: 'unavailable',
  AVAILABLE: 'available',
} as const;

/**
 * Available levels of job support.
 */
export type JobSupport = (typeof JobSupport)[keyof typeof JobSupport];

/**
 * Array of operations to perform
 */
export type JsonPatchDocument = Array<JsonPatchOperation>;

/**
 * Describes a single operation in a JSON Patch document. Includes the operation type, the target property path, and the value to be used.
 */
export type JsonPatchOperation = {
  /**
   * The operation type. Allowed values: 'add', 'remove', 'replace', 'move', 'copy', 'test'.
   */
  op: 'add' | 'remove' | 'replace' | 'move' | 'copy' | 'test';
  /**
   * The JSON Pointer path to the property in the target document where the operation is to be applied.
   */
  path: string;
  /**
   * Should be a path, required when using move, copy
   */
  from?: string;
  /**
   * The value to apply for 'add', 'replace', or 'test' operations. Not required for 'remove', 'move', or 'copy'.
   */
  value?:
    | string
    | number
    | boolean
    | {
        [key: string]: unknown;
      }
    | Array<unknown>
    | null;
};

export type KlingCameraControl = {
  config?: KlingCameraControlConfig;
};

export type KlingCameraControlConfig = {
  /**
   * Horizontal, controls the camera's movement along the horizontal axis (translation along the x-axis).
   */
  horizontal?: number | null;
  /**
   * Vertical, controls the camera's movement along the vertical axis (translation along the y-axis).
   */
  vertical?: number | null;
  /**
   * Pan, controls the camera's rotation in the horizontal plane (rotation around the y-axis).
   */
  pan?: number | null;
  /**
   * Tilt, controls the camera's rotation in the horizontal plane (rotation around the y-axis).
   */
  tilt?: number | null;
  /**
   * Roll, controls the camera's rolling amount (rotation around the z-axis).
   */
  roll?: number | null;
  /**
   * Zoom, controls the change in the camera's focal length, affecting the proximity of the field of view.
   */
  zoom?: number | null;
};

export const KlingMode = {
  STANDARD: 'standard',
  PROFESSIONAL: 'professional',
} as const;

export type KlingMode = (typeof KlingMode)[keyof typeof KlingMode];

export const KlingModel = {
  V1: 'v1',
  V1_5: 'v1.5',
  V1_6: 'v1.6',
  V2: 'v2',
  V2_5_TURBO: 'v2.5-turbo',
} as const;

export type KlingModel = (typeof KlingModel)[keyof typeof KlingModel];

export const KlingVideoGenAspectRatio = {
  '16:9': '16:9',
  '9:16': '9:16',
  '1:1': '1:1',
} as const;

export type KlingVideoGenAspectRatio =
  (typeof KlingVideoGenAspectRatio)[keyof typeof KlingVideoGenAspectRatio];

export const KlingVideoGenDuration = {
  5: '5',
  10: '10',
} as const;

export type KlingVideoGenDuration =
  (typeof KlingVideoGenDuration)[keyof typeof KlingVideoGenDuration];

export type KlingVideoGenInput = VideoGenInput & {
  engine: 'kling';
} & {
  model?: KlingModel;
  negativePrompt?: string | null;
  cfgScale?: number;
  mode?: KlingMode;
  aspectRatio?: KlingVideoGenAspectRatio;
  duration?: KlingVideoGenDuration;
  cameraControl?: KlingCameraControl;
  sourceImageUrl?: string | null;
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  sourceImage?: string | null;
} & {
  engine: 'kling';
};

export type KohyaImageResourceTrainingInput = ImageResourceTrainingInput & {
  engine: 'kohya';
} & {
  /**
   * An epoch is one set of learning. By default, we will save a maximum of 20 epochs (evenly distributed), and they are all available for download.
   */
  maxTrainEpochs?: number;
  /**
   * Num Repeats defines how many times each individual image gets put into VRAM. As opposed to batch size, which is how many images are placed into VRAM at once.
   */
  numRepeats?: number;
  /**
   * Batch size is the number of images that will be placed into VRAM at once. A batch size of 2 will train two images at a time, simultaneously.
   */
  trainBatchSize?: number | null;
  /**
   * Specify the maximum resolution of training images. If the training images exceed the resolution specified here, they will be scaled down to this resolution
   */
  resolution?: number | null;
  /**
   * Sorts images into buckets by size for the purposes of training. If your training images are all the same size, you can turn this option off, but leaving it on has no effect.
   */
  enableBucket?: boolean;
  /**
   * Randomly changes the order of your tags during training. The intent of shuffling is to improve learning. If you are using captions (sentences), this option has no meaning.
   */
  shuffleCaption?: boolean;
  /**
   * If your training images have tags, you can randomly shuffle them.
   * However, if you have words that you want to keep at the beginning, you can use this option to specify "Keep the first 0 words at the beginning".
   * This option does nothing if the Shuffle Tags option is off.
   */
  keepTokens?: number;
  /**
   * Determines which layer's vector output will be used. There are 12 layers, and setting the skip will select "xth from the end" of the total layers. For anime, we use 2. For everything else, 1.
   */
  clipSkip?: number;
  /**
   * If this option is turned on, the image will be horizontally flipped randomly. It can learn left and right angles, which is useful when you want to learn symmetrical people and objects.
   */
  flipAugmentation?: boolean;
  /**
   * Sets the learning rate for U-Net. This is the learning rate when performing additional learning on each attention block (and other blocks depending on the setting) in U-Net
   */
  unetLR?: number;
  /**
   * Sets the learning rate for the text encoder. The effect of additional training on text encoders affects the entire U-Net.
   */
  textEncoderLR?: number;
  /**
   * You can change the learning rate in the middle of learning. A scheduler is a setting for how to change the learning rate.
   */
  lrScheduler?: 'constant' | 'cosine' | 'cosine_with_restarts' | 'linear';
  /**
   * This option specifies how many cycles the scheduler runs during training. It is only used when "cosine_with_restarts" or "polynomial" is used as the scheduler.
   */
  lrSchedulerNumCycles?: number;
  /**
   * Learning is performed by putting noise of various strengths on the training image,
   * but depending on the difference in strength of the noise on which it is placed, learning will be
   * stable by moving closer to or farther from the learning target.
   *
   * Min SNR gamma was introduced to compensate for that. When learning images have little noise,
   * it may deviate greatly from the target, so try to suppress this jump.
   */
  minSnrGamma?: number | null;
  /**
   * The larger the Dim setting, the more learning information can be stored, but the possibility of learning unnecessary information other than the learning target increases. A larger Dim also increases LoRA file size.
   */
  networkDim?: number | null;
  /**
   * The smaller the Network alpha value, the larger the stored LoRA neural net weights.
   * For example, with an Alpha of 16 and a Dim of 32, the strength of the weight used is 16/32 = 0.5,
   * meaning that the learning rate is only half as powerful as the Learning Rate setting.
   *
   * If Alpha and Dim are the same number, the strength used will be 1 and will have no effect on the learning rate.
   */
  networkAlpha?: number | null;
  /**
   * Adds noise to training images. 0 adds no noise at all. A value of 1 adds strong noise.
   */
  noiseOffset?: number | null;
  /**
   * The optimizer determines how to update the neural net weights during training.
   * Various methods have been proposed for smart learning, but the most commonly used in LoRA learning
   * is "AdamW8bit" or "Adafactor" for SDXL.
   */
  optimizerType?: string | null;
  readonly targetSteps?: number | null;
} & {
  engine: 'kohya';
};

export const LeresBoost = {
  DISABLE: 'disable',
  ENABLE: 'enable',
} as const;

export type LeresBoost = (typeof LeresBoost)[keyof typeof LeresBoost];

export const LightricksAspectRatio = {
  '1:1': '1:1',
  '16:9': '16:9',
  '9:16': '9:16',
  '3:2': '3:2',
  '2:3': '2:3',
} as const;

export type LightricksAspectRatio =
  (typeof LightricksAspectRatio)[keyof typeof LightricksAspectRatio];

export type LightricksVideoGenInput = VideoGenInput & {
  engine: 'lightricks';
} & {
  negativePrompt?: string | null;
  cfgScale?: number;
  frameRate?: number;
  duration?: number;
  seed?: number | null;
  steps?: number;
  aspectRatio?: LightricksAspectRatio;
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  sourceImage?: string | null;
  expandPrompt?: boolean;
} & {
  engine: 'lightricks';
};

/**
 * Represents the input information needed for the MediaHash workflow step.
 */
export type MediaHashInput = {
  mediaUrl: string;
  /**
   * The types of hashes to generate.
   */
  hashTypes: Array<MediaHashType>;
};

/**
 * Represents the output information returned from the MediaHash workflow step.
 */
export type MediaHashOutput = {
  /**
   * The generated hashes, keyed by hash type (e.g., "perceptual" -> "12345678").
   */
  hashes: {
    [key: string]: string;
  };
};

/**
 * MediaHash
 */
export type MediaHashStep = WorkflowStep & {
  $type: 'mediaHash';
} & {
  input: MediaHashInput;
  output?: MediaHashOutput;
} & {
  $type: 'mediaHash';
};

/**
 * MediaHash
 */
export type MediaHashStepTemplate = WorkflowStepTemplate & {
  $type: 'mediaHash';
} & {
  input: MediaHashInput;
} & {
  $type: 'mediaHash';
};

/**
 * Represents the type of hash algorithm to use for media content.
 */
export const MediaHashType = {
  PERCEPTUAL: 'perceptual',
} as const;

/**
 * Represents the type of hash algorithm to use for media content.
 */
export type MediaHashType = (typeof MediaHashType)[keyof typeof MediaHashType];

/**
 * Represents the input information needed for the MediaRating workflow step.
 */
export type MediaRatingInput = {
  mediaUrl: string;
  /**
   * The engine to use for media rating. Valid values: "default" (HiveVLM) or "civitai".
   */
  engine?: string;
};

/**
 * Represents the output information returned from the MediaRating workflow step.
 */
export type MediaRatingOutput = {
  nsfwLevel: NsfwLevel;
  /**
   * The reason the content was blocked, if any.
   */
  blockedReason?: string | null;
  /**
   * Whether the content is blocked.
   */
  isBlocked: boolean;
  /**
   * Detected content labels (e.g., "Animal", "Child", etc.).
   */
  labels?: Array<string> | null;
};

/**
 * MediaRating
 */
export type MediaRatingStep = WorkflowStep & {
  $type: 'mediaRating';
} & {
  input: MediaRatingInput;
  output?: MediaRatingOutput;
} & {
  $type: 'mediaRating';
};

/**
 * MediaRating
 */
export type MediaRatingStepTemplate = WorkflowStepTemplate & {
  $type: 'mediaRating';
} & {
  input: MediaRatingInput;
} & {
  $type: 'mediaRating';
};

export const Metric3dBackbone = {
  VIT_SMALL: 'vit-small',
  VIT_LARGE: 'vit-large',
  VIT_GIANT2: 'vit-giant2',
} as const;

export type Metric3dBackbone = (typeof Metric3dBackbone)[keyof typeof Metric3dBackbone];

export type MiniMaxVideoGenInput = VideoGenInput & {
  engine: 'minimax';
} & {
  model?: MiniMaxVideoGenModel;
  enablePromptEnhancer?: boolean;
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  sourceImage?: string | null;
} & {
  engine: 'minimax';
};

export const MiniMaxVideoGenModel = {
  HAILOU: 'hailou',
} as const;

export type MiniMaxVideoGenModel = (typeof MiniMaxVideoGenModel)[keyof typeof MiniMaxVideoGenModel];

export type MochiVideoGenInput = VideoGenInput & {
  engine: 'mochi';
} & {
  seed?: number;
  enablePromptEnhancer?: boolean;
} & {
  engine: 'mochi';
};

export type MusubiImageResourceTrainingInput = ImageResourceTrainingInput & {
  engine: 'musubi';
} & {
  /**
   * Number of training epochs. An epoch is one complete pass through the training dataset.
   * Maximum of 20 epochs can be specified.
   */
  epochs?: number;
  /**
   * Specify the maximum resolution of training images. If the training images exceed the resolution specified here, they will be scaled down to this resolution
   */
  resolution?: number | null;
  /**
   * Sorts images into buckets by size for the purposes of training. If your training images are all the same size, you can turn this option off, but leaving it on has no effect.
   */
  enableBucket?: boolean;
  /**
   * Sets the learning rate for U-Net. This is the learning rate when performing additional learning on each attention block (and other blocks depending on the setting) in U-Net
   */
  unetLR?: number;
  /**
   * You can change the learning rate in the middle of learning. A scheduler is a setting for how to change the learning rate.
   */
  lrScheduler?: 'constant' | 'cosine' | 'cosine_with_restarts' | 'linear';
  /**
   * This option specifies how many cycles the scheduler runs during training. It is only used when "cosine_with_restarts" or "polynomial" is used as the scheduler.
   */
  lrSchedulerNumCycles?: number;
  /**
   * The larger the Dim setting, the more learning information can be stored, but the possibility of learning unnecessary information other than the learning target increases. A larger Dim also increases LoRA file size.
   */
  networkDim?: number | null;
  /**
   * The smaller the Network alpha value, the larger the stored LoRA neural net weights.
   * For example, with an Alpha of 16 and a Dim of 32, the strength of the weight used is 16/32 = 0.5,
   * meaning that the learning rate is only half as powerful as the Learning Rate setting.
   *
   * If Alpha and Dim are the same number, the strength used will be 1 and will have no effect on the learning rate.
   */
  networkAlpha?: number | null;
  /**
   * The optimizer determines how to update the neural net weights during training.
   * Various methods have been proposed for smart learning, but the most commonly used in LoRA learning
   * is "AdamW8bit" or "Adafactor" for SDXL.
   */
  optimizerType?: string | null;
  readonly targetSteps?: number | null;
} & {
  engine: 'musubi';
};

export const NsfwLevel = {
  PG: 'pg',
  PG13: 'pg13',
  R: 'r',
  X: 'x',
  XXX: 'xxx',
  NA: 'na',
} as const;

export type NsfwLevel = (typeof NsfwLevel)[keyof typeof NsfwLevel];

export type NanoBananaProImageGenInput = GoogleImageGenInput & {
  prompt: string;
  aspectRatio?: '21:9' | '16:9' | '3:2' | '4:3' | '5:4' | '1:1' | '4:5' | '3:4' | '2:3' | '9:16';
  numImages?: number;
  resolution?: '1K' | '2K' | '4K';
  outputFormat?: 'jpeg' | 'png' | 'webp';
  images?: Array<string>;
} & {
  model: 'nano-banana-pro';
};

export type OpenAiDallE2CreateImageGenInput = OpenAiDallE2ImageGenInput & {
  background?: 'auto' | 'transparent' | 'opaque';
} & {
  operation: 'createImage';
};

export type OpenAiDallE2EditImageInput = OpenAiDallE2ImageGenInput & {
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  image: string;
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  mask?: string | null;
} & {
  operation: 'editImage';
};

export type OpenAiDallE2ImageGenInput = OpenApiImageGenInput & {
  operation: string;
  prompt: string;
  size: '256x256' | '512x512' | '1024x1024';
  quantity?: number;
} & {
  model: 'dall-e-2';
};

export type OpenAiDallE3CreateImageGenInput = OpenAiDallE3ImageGenInput & {
  background?: 'auto' | 'transparent' | 'opaque';
} & {
  operation: 'createImage';
};

export type OpenAiDallE3ImageGenInput = OpenApiImageGenInput & {
  operation: string;
  prompt: string;
  size: '1024x1024' | '1792x1024' | '1024x1792';
  style?: 'natural' | 'vivid';
  quality?: 'auto' | 'hd' | 'standard';
} & {
  model: 'dall-e-3';
};

export type OpenAiGpt1CreateImageInput = OpenAiGpt1ImageGenInput & {} & {
  operation: 'createImage';
};

export type OpenAiGpt1EditImageInput = OpenAiGpt1ImageGenInput & {
  images: Array<string>;
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  mask?: string | null;
} & {
  operation: 'editImage';
};

export type OpenAiGpt1ImageGenInput = OpenApiImageGenInput & {
  operation: string;
  prompt: string;
  size?: '1024x1024' | '1536x1024' | '1024x1536';
  quantity?: number;
  background?: 'auto' | 'transparent' | 'opaque';
  quality?: 'auto' | 'high' | 'medium' | 'low';
} & {
  model: 'gpt-image-1';
};

export type OpenApiImageGenInput = ImageGenInput & {
  engine: 'openai';
} & {
  model: string;
  prompt: string;
} & {
  engine: 'openai';
};

export const OutputFormat = {
  PNG: 'png',
  JPEG: 'jpeg',
  WEB_P: 'webP',
} as const;

export type OutputFormat = (typeof OutputFormat)[keyof typeof OutputFormat];

export type PreprocessImageAnimalPoseInput = PreprocessImageInput & {
  kind: 'animal-pose';
} & {
  bboxDetector?: AnimalPoseBboxDetector;
  poseEstimator?: AnimalPoseEstimator;
} & {
  kind: 'animal-pose';
};

export type PreprocessImageAnimeLineartInput = PreprocessImageInput & {
  kind: 'lineart-anime';
} & {} & {
  kind: 'lineart-anime';
};

export type PreprocessImageAnylineInput = PreprocessImageInput & {
  kind: 'anyline';
} & {
  mergeWithLineart?: AnylineMergeWith;
  lineartLowerBound?: number;
  lineartUpperBound?: number;
  objectMinSize?: number;
  objectConnectivity?: number;
} & {
  kind: 'anyline';
};

export type PreprocessImageBaeNormalInput = PreprocessImageInput & {
  kind: 'bae-normal';
} & {} & {
  kind: 'bae-normal';
};

export type PreprocessImageBinaryInput = PreprocessImageInput & {
  kind: 'binary';
} & {
  binThreshold?: number;
} & {
  kind: 'binary';
};

export type PreprocessImageCannyInput = PreprocessImageInput & {
  kind: 'canny';
} & {
  lowThreshold?: number;
  highThreshold?: number;
} & {
  kind: 'canny';
};

export type PreprocessImageColorInput = PreprocessImageInput & {
  kind: 'color';
} & {} & {
  kind: 'color';
};

export type PreprocessImageDensePoseInput = PreprocessImageInput & {
  kind: 'densepose';
} & {
  model?: DensePoseModel;
  colormap?: DensePoseColormap;
} & {
  kind: 'densepose';
};

export type PreprocessImageDepthAnythingInput = PreprocessImageInput & {
  kind: 'depth-anything';
} & {
  checkpoint?: DepthAnythingCheckpoint;
} & {
  kind: 'depth-anything';
};

export type PreprocessImageDepthAnythingV2Input = PreprocessImageInput & {
  kind: 'depth-anything-v2';
} & {
  checkpoint?: DepthAnythingV2Checkpoint;
} & {
  kind: 'depth-anything-v2';
};

export type PreprocessImageDsineNormalInput = PreprocessImageInput & {
  kind: 'dsine-normal';
} & {
  fov?: number;
  iterations?: number;
} & {
  kind: 'dsine-normal';
};

export type PreprocessImageDwPoseInput = PreprocessImageInput & {
  kind: 'dwpose';
} & {
  detectHand?: boolean;
  detectBody?: boolean;
  detectFace?: boolean;
  bboxDetector?: DwPoseBboxDetector;
  poseEstimator?: DwPoseEstimator;
} & {
  kind: 'dwpose';
};

export type PreprocessImageFakeScribbleInput = PreprocessImageInput & {
  kind: 'fake-scribble';
} & {
  safe?: SafeMode;
} & {
  kind: 'fake-scribble';
};

export type PreprocessImageHedInput = PreprocessImageInput & {
  kind: 'hed';
} & {
  safe?: SafeMode;
} & {
  kind: 'hed';
};

export type PreprocessImageInput = {
  kind: string;
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  image: string;
  resolution?: number;
  /**
   * Gets the preprocessor type identifier used to map to ComfyUI nodes.
   * This is derived from the JsonDerivedType discriminator.
   */
  preprocessorType?: string;
};

export type PreprocessImageLeresDepthInput = PreprocessImageInput & {
  kind: 'leres-depth';
} & {
  removeNearest?: number;
  removeBackground?: number;
  boost?: LeresBoost;
} & {
  kind: 'leres-depth';
};

export type PreprocessImageMangaLineartInput = PreprocessImageInput & {
  kind: 'lineart-manga';
} & {} & {
  kind: 'lineart-manga';
};

export type PreprocessImageMediaPipeFaceInput = PreprocessImageInput & {
  kind: 'mediapipe-face';
} & {
  maxFaces?: number;
  minConfidence?: number;
} & {
  kind: 'mediapipe-face';
};

export type PreprocessImageMetric3dDepthInput = PreprocessImageInput & {
  kind: 'metric3d-depth';
} & {
  backbone?: Metric3dBackbone;
  fx?: number;
  fy?: number;
} & {
  kind: 'metric3d-depth';
};

export type PreprocessImageMetric3dNormalInput = PreprocessImageInput & {
  kind: 'metric3d-normal';
} & {
  backbone?: Metric3dBackbone;
  fx?: number;
  fy?: number;
} & {
  kind: 'metric3d-normal';
};

export type PreprocessImageMidasDepthInput = PreprocessImageInput & {
  kind: 'midas-depth';
} & {
  a?: number;
  backgroundThreshold?: number;
} & {
  kind: 'midas-depth';
};

export type PreprocessImageMidasNormalInput = PreprocessImageInput & {
  kind: 'midas-normal';
} & {
  a?: number;
  backgroundThreshold?: number;
} & {
  kind: 'midas-normal';
};

export type PreprocessImageMlsdInput = PreprocessImageInput & {
  kind: 'mlsd';
} & {
  scoreThreshold?: number;
  distanceThreshold?: number;
} & {
  kind: 'mlsd';
};

export type PreprocessImageOneFormerAde20kInput = PreprocessImageInput & {
  kind: 'oneformer-ade20k';
} & {} & {
  kind: 'oneformer-ade20k';
};

export type PreprocessImageOneFormerCocoInput = PreprocessImageInput & {
  kind: 'oneformer-coco';
} & {} & {
  kind: 'oneformer-coco';
};

export type PreprocessImageOpenPoseInput = PreprocessImageInput & {
  kind: 'openpose';
} & {
  detectHand?: boolean;
  detectBody?: boolean;
  detectFace?: boolean;
} & {
  kind: 'openpose';
};

export type PreprocessImageOutput = {
  blob: ImageBlob;
};

export type PreprocessImagePidinetInput = PreprocessImageInput & {
  kind: 'pidinet';
} & {
  safe?: SafeMode;
} & {
  kind: 'pidinet';
};

export type PreprocessImageRealisticLineartInput = PreprocessImageInput & {
  kind: 'lineart-realistic';
} & {
  coarse?: CoarseMode;
} & {
  kind: 'lineart-realistic';
};

export type PreprocessImageScribbleInput = PreprocessImageInput & {
  kind: 'scribble';
} & {} & {
  kind: 'scribble';
};

export type PreprocessImageScribblePidinetInput = PreprocessImageInput & {
  kind: 'scribble-pidinet';
} & {
  safe?: SafeMode;
} & {
  kind: 'scribble-pidinet';
};

export type PreprocessImageScribbleXdogInput = PreprocessImageInput & {
  kind: 'scribble-xdog';
} & {
  threshold?: number;
} & {
  kind: 'scribble-xdog';
};

export type PreprocessImageShuffleInput = PreprocessImageInput & {
  kind: 'shuffle';
} & {
  seed?: number;
} & {
  kind: 'shuffle';
};

export type PreprocessImageStandardLineartInput = PreprocessImageInput & {
  kind: 'lineart-standard';
} & {
  gaussianSigma?: number;
  intensityThreshold?: number;
} & {
  kind: 'lineart-standard';
};

export type PreprocessImageStep = WorkflowStep & {
  $type: 'preprocessImage';
} & {
  input: PreprocessImageInput;
  output?: PreprocessImageOutput;
} & {
  $type: 'preprocessImage';
};

export type PreprocessImageStepTemplate = WorkflowStepTemplate & {
  $type: 'preprocessImage';
} & {
  input: PreprocessImageInput;
} & {
  $type: 'preprocessImage';
};

export type PreprocessImageTeedInput = PreprocessImageInput & {
  kind: 'teed';
} & {
  safeSteps?: number;
} & {
  kind: 'teed';
};

export type PreprocessImageTileInput = PreprocessImageInput & {
  kind: 'tile';
} & {
  pyrUpIterations?: number;
} & {
  kind: 'tile';
};

export type PreprocessImageUniFormerInput = PreprocessImageInput & {
  kind: 'uniformer';
} & {} & {
  kind: 'uniformer';
};

export type PreprocessImageZoeDepthAnythingInput = PreprocessImageInput & {
  kind: 'zoe-depth-anything';
} & {
  environment?: ZoeDepthEnvironment;
} & {
  kind: 'zoe-depth-anything';
};

export type PreprocessImageZoeDepthInput = PreprocessImageInput & {
  kind: 'zoe-depth';
} & {} & {
  kind: 'zoe-depth';
};

/**
 * Available options for priority.
 */
export const Priority = {
  HIGH: 'high',
  NORMAL: 'normal',
  LOW: 'low',
} as const;

/**
 * Available options for priority.
 */
export type Priority = (typeof Priority)[keyof typeof Priority];

export type ProblemDetails = {
  type?: string | null;
  title?: string | null;
  status?: number | null;
  detail?: string | null;
  instance?: string | null;
  [key: string]:
    | unknown
    | (string | null)
    | (string | null)
    | (number | null)
    | (string | null)
    | (string | null)
    | undefined;
};

export type Qwen20bCreateImageGenInput = Qwen20bImageGenInput & {
  width?: number;
  height?: number;
} & {
  operation: 'createImage';
};

export type Qwen20bEditImageGenInput = Qwen20bImageGenInput & {
  images: Array<string>;
  readonly width?: number;
  readonly height?: number;
} & {
  operation: 'editImage';
};

export type Qwen20bImageGenInput = QwenImageGenInput & {
  operation: string;
  diffuserModel?: string;
  prompt: string;
  negativePrompt?: string | null;
  sampleMethod?: SdCppSampleMethod;
  schedule?: SdCppSchedule;
  steps?: number;
  cfgScale?: number;
  seed?: number | null;
  quantity?: number;
} & {
  model: '20b';
};

export type Qwen20bVariantImageGenInput = Qwen20bImageGenInput & {
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  image: string;
  strength?: number;
  readonly width?: number;
  readonly height?: number;
} & {
  operation: 'createVariant';
};

/**
 * AI Toolkit training for Qwen Image models
 */
export type QwenAiToolkitTrainingInput = AiToolkitTrainingInput & {} & {
  ecosystem: 'qwen';
};

export type QwenImageGenInput = SdCppImageGenInput & {
  model: string;
} & {
  ecosystem: 'qwen';
};

/**
 * Represents the "for" clause in a repeat step that specifies what to iterate over.
 */
export type RepeatForClause = {
  /**
   * The dynamic assignment reference to the array to iterate over (e.g., "$stepName").
   */
  $ref: string;
  /**
   * The path to the array property in the referenced step's output (e.g., "output.frames").
   */
  path: string;
  /**
   * The variable name to use when referencing the current iteration item in template steps (e.g., "frame").
   */
  as: string;
};

/**
 * Represents the input information needed for the Repeat workflow step.
 */
export type RepeatInput = {
  for: RepeatForClause;
  template: WorkflowStepTemplate;
};

/**
 * Represents the output information returned from the Repeat workflow step.
 */
export type RepeatOutput = {
  steps: Array<WorkflowStep>;
};

/**
 * Details for a specific resource.
 */
export type ResourceInfo = {
  /**
   * An AIR ID for the resource.
   */
  air: string;
  /**
   * The resource size in bytes.
   */
  size: number;
  /**
   * A collection of hashes.
   */
  hashes: {
    [key: string]: string;
  };
  /**
   * An array of download urls.
   */
  downloadUrls: Array<string>;
  /**
   * The name of the resource.
   */
  resourceName?: string | null;
  /**
   * The name of the version.
   */
  versionName?: string | null;
  /**
   * The date time to invalidate at.
   */
  invalidateAt?: string | null;
  /**
   * A DateTime representing when early access for the resource ends.
   */
  earlyAccessEndsAt?: string | null;
  /**
   * A bool indicating if permission is required to use this resource.
   */
  checkPermission?: boolean;
  /**
   * A bool indicating if generation is enabled for this resource.
   */
  canGenerate?: boolean;
  /**
   * An optional limit on the number of uses for this resource per user that has early acccess.
   */
  freeTrialLimit?: number | null;
  /**
   * Wether this resource requires authorization.
   */
  requiresAuthorization?: boolean | null;
  fileFormat?: FileFormat;
  /**
   * A boolean indicating whether this resource restricts mature content generation.
   * If resources with this restriction are used in generation, then generations will automatically be enforced to not generate mature content
   */
  hasMatureContentRestriction?: boolean;
  /**
   * Get a rank between 0-1 on the popularity of the resource.
   */
  popularityRank?: number | null;
  /**
   * Get wether this resource is featured
   */
  isFeatured?: boolean | null;
  /**
   * The date at which this model got published
   */
  publishedAt?: string | null;
  /**
   * A boolean indicating whether this resource restricts to SFW content generation.
   * NSFWContent covers X and AA whereas MatureContent includes R rated content.
   */
  hasNSFWContentRestriction?: boolean;
};

/**
 * AI Toolkit training for Stable Diffusion 1.5 models
 */
export type Sd1AiToolkitTrainingInput = AiToolkitTrainingInput & {
  /**
   * Learning is performed by putting noise of various strengths on the training image,
   * but depending on the difference in strength of the noise on which it is placed, learning will be
   * stable by moving closer to or farther from the learning target.
   *
   * Min SNR gamma was introduced to compensate for that. When learning images have little noise,
   * it may deviate greatly from the target, so try to suppress this jump.
   */
  minSnrGamma?: number | null;
  /**
   * The primary model to train upon.
   */
  model?: string;
} & {
  ecosystem: 'sd1';
};

export const SafeMode = {
  ENABLE: 'enable',
  DISABLE: 'disable',
} as const;

export type SafeMode = (typeof SafeMode)[keyof typeof SafeMode];

/**
 * The available options for schedulers used in image generation.
 */
export const Scheduler = {
  EULER_A: 'eulerA',
  EULER: 'euler',
  LMS: 'lms',
  HEUN: 'heun',
  DP_M2: 'dpM2',
  DP_M2A: 'dpM2A',
  DP_M2SA: 'dpM2SA',
  DP_M2M: 'dpM2M',
  DPMSDE: 'dpmsde',
  DPM_FAST: 'dpmFast',
  DPM_ADAPTIVE: 'dpmAdaptive',
  LMS_KARRAS: 'lmsKarras',
  DP_M2_KARRAS: 'dpM2Karras',
  DP_M2A_KARRAS: 'dpM2AKarras',
  DP_M2SA_KARRAS: 'dpM2SAKarras',
  DP_M2M_KARRAS: 'dpM2MKarras',
  DPMSDE_KARRAS: 'dpmsdeKarras',
  DDIM: 'ddim',
  PLMS: 'plms',
  UNI_PC: 'uniPC',
  UNDEFINED: 'undefined',
  LCM: 'lcm',
  DDPM: 'ddpm',
  DEIS: 'deis',
  DP_M3MSDE: 'dpM3MSDE',
} as const;

/**
 * The available options for schedulers used in image generation.
 */
export type Scheduler = (typeof Scheduler)[keyof typeof Scheduler];

export type SdCppImageGenInput = ImageGenInput & {
  engine: 'sdcpp';
} & {
  ecosystem: string;
} & {
  engine: 'sdcpp';
};

export const SdCppSampleMethod = {
  EULER: 'euler',
  HEUN: 'heun',
  DPM2: 'dpm2',
  'DPM++2S_A': 'dpm++2s_a',
  'DPM++2M': 'dpm++2m',
  'DPM++2MV2': 'dpm++2mv2',
  IPNDM: 'ipndm',
  IPNDM_V: 'ipndm_v',
  DDIM_TRAILING: 'ddim_trailing',
  EULER_A: 'euler_a',
  LCM: 'lcm',
} as const;

export type SdCppSampleMethod = (typeof SdCppSampleMethod)[keyof typeof SdCppSampleMethod];

export const SdCppSchedule = {
  SIMPLE: 'simple',
  DISCRETE: 'discrete',
  KARRAS: 'karras',
  EXPONENTIAL: 'exponential',
  AYS: 'ays',
} as const;

export type SdCppSchedule = (typeof SdCppSchedule)[keyof typeof SdCppSchedule];

/**
 * AI Toolkit training for Stable Diffusion XL models
 */
export type SdxlAiToolkitTrainingInput = AiToolkitTrainingInput & {
  /**
   * Learning is performed by putting noise of various strengths on the training image,
   * but depending on the difference in strength of the noise on which it is placed, learning will be
   * stable by moving closer to or farther from the learning target.
   *
   * Min SNR gamma was introduced to compensate for that. When learning images have little noise,
   * it may deviate greatly from the target, so try to suppress this jump.
   */
  minSnrGamma?: number | null;
  /**
   * The primary model to train upon.
   */
  model?: string;
} & {
  ecosystem: 'sdxl';
};

export type SeedreamImageGenInput = ImageGenInput & {
  engine: 'seedream';
} & {
  prompt: string;
  quantity?: number;
  width?: number;
  height?: number;
  guidanceScale?: number;
  seed?: number | null;
  enableSafetyChecker?: boolean;
  version?: SeedreamVersion;
  images?: Array<string>;
} & {
  engine: 'seedream';
};

export const SeedreamVersion = {
  V3: 'v3',
  V4: 'v4',
  V4_5: 'v4.5',
} as const;

export type SeedreamVersion = (typeof SeedreamVersion)[keyof typeof SeedreamVersion];

/**
 * Sora 2 Image-to-Video
 * FAL Endpoints:
 * - Standard: https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/sora-2/image-to-video
 * - Pro: https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/sora-2/image-to-video/pro
 */
export type Sora2ImageToVideoInput = SoraVideoGenInput & {
  images?: Array<string>;
} & {
  operation: 'image-to-video';
};

/**
 * Sora 2 Text-to-Video
 * FAL Endpoints:
 * - Standard: https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/sora-2/text-to-video
 * - Pro: https://fal.ai/api/openapi/queue/openapi.json?endpoint_id=fal-ai/sora-2/text-to-video/pro
 */
export type Sora2TextToVideoInput = SoraVideoGenInput & {} & {
  operation: 'text-to-video';
};

/**
 * Base class for Sora 2 video generation (OpenAI's Sora-2 model via FAL)
 * Since FAL has a one-to-one mapping with OpenAI's Sora API, we don't need a provider layer.
 * Discriminator: operation (text-to-video or image-to-video)
 */
export type SoraVideoGenInput = VideoGenInput & {
  engine: 'sora';
} & {
  operation: string | null;
  duration?: number;
  seed?: number | null;
  resolution?: '720p' | '1080p';
  aspectRatio?: 'auto' | '16:9' | '9:16';
  usePro?: boolean;
} & {
  engine: 'sora';
};

/**
 * Input for an text to image step.
 */
export type TextToImageInput = {
  /**
   * The number of batches to run.
   */
  quantity?: number;
  /**
   * The size of each batch
   */
  batchSize?: number;
  /**
   * The AIR of the checkpoint model to use for generation.
   */
  model?: string;
  /**
   * Get or set a associative list of additional networks. Use the AIR of the network as the key.
   */
  additionalNetworks?: {
    [key: string]: ImageJobNetworkParams;
  };
  /**
   * Get or set a associative list of ControlNets.
   */
  controlNets?: Array<ImageJobControlNet>;
  /**
   * The provided text prompt.
   */
  prompt: string;
  /**
   * The provided negative text prompt.
   */
  negativePrompt?: string | null;
  scheduler?: Scheduler;
  /**
   * The number of steps for image generation.
   */
  steps?: number;
  /**
   * The CFG scale value for image generation.
   */
  cfgScale?: number;
  /**
   * The desired image width in pixels.
   */
  width: number;
  /**
   * The desired image height in pixels.
   */
  height: number;
  /**
   * The seed to use in image generation. Defaults to a random value if left unpopulated.
   */
  seed?: number;
  /**
   * The clip skip value for image generation.
   */
  clipSkip?: number;
  /**
   * External metadata that will be stored with the image
   */
  imageMetadata?: string | null;
  /**
   * An optional engine to use for generation.
   */
  engine?: string | null;
  outputFormat?: OutputFormat;
};

/**
 * Represents the output of a TextToImage workflow step.
 */
export type TextToImageOutput = {
  /**
   * A collection of output images.
   */
  images: Array<ImageBlob>;
};

/**
 * TextToImage
 */
export type TextToImageStep = WorkflowStep & {
  $type: 'textToImage';
} & {
  input: TextToImageInput;
  output?: TextToImageOutput;
} & {
  $type: 'textToImage';
};

/**
 * TextToImage
 */
export type TextToImageStepTemplate = WorkflowStepTemplate & {
  $type: 'textToImage';
} & {
  input: TextToImageInput;
} & {
  $type: 'textToImage';
};

/**
 * Represents training data in various formats
 */
export type TrainingData = {
  type: string;
};

/**
 * Input for a training step.
 */
export type TrainingInput = {
  engine: string;
  trainingData: TrainingData;
  samples?: TrainingInputSamples;
};

/**
 * Sample generation configuration for training workflows
 */
export type TrainingInputSamples = {
  /**
   * A selection of sample prompts to generate preview outputs during training.
   */
  prompts?: Array<string>;
  /**
   * An optional negative prompt that will be applied when generating samples
   */
  negativePrompt?: string | null;
};

/**
 * The moderation status of the training data
 */
export const TrainingModerationStatus = {
  EVALUATING: 'evaluating',
  UNDER_REVIEW: 'underReview',
  APPROVED: 'approved',
  REJECTED: 'rejected',
} as const;

/**
 * The moderation status of the training data
 */
export type TrainingModerationStatus =
  (typeof TrainingModerationStatus)[keyof typeof TrainingModerationStatus];

/**
 * Output from a training step.
 */
export type TrainingOutput = {
  moderationStatus?: TrainingModerationStatus;
  /**
   * The trained model artifacts for each epoch
   */
  epochs?: Array<TrainingOutputEpochResult>;
};

/**
 * Represents the output of a single training epoch
 */
export type TrainingOutputEpochResult = {
  /**
   * The epoch number (1-based)
   */
  epochNumber?: number;
  model: Blob;
  /**
   * Sample outputs (images/videos/audio) generated with this epoch's model
   */
  samples?: Array<Blob>;
};

/**
 * Training
 */
export type TrainingStep = WorkflowStep & {
  $type: 'training';
} & {
  input: TrainingInput;
  output?: TrainingOutput;
} & {
  $type: 'training';
};

/**
 * Training
 */
export type TrainingStepTemplate = WorkflowStepTemplate & {
  $type: 'training';
} & {
  input: TrainingInput;
} & {
  $type: 'training';
};

/**
 * Transaction information.
 */
export type TransactionInfo = {
  type: TransactionType;
  /**
   * The transaction amount.
   */
  amount: number;
  /**
   * The transaction ID.
   */
  id?: string | null;
  accountType?: BuzzClientAccount;
};

export type TransactionSummary = {
  /**
   * Get a list of individual transactions.
   */
  list?: Array<TransactionInfo>;
  /**
   * A boolean returned with whatif requests to indicate whether the user has nsufficient buzz to run a workflow.
   */
  insufficientBuzz?: boolean | null;
};

export const TransactionType = {
  DEBIT: 'debit',
  CREDIT: 'credit',
} as const;

export type TransactionType = (typeof TransactionType)[keyof typeof TransactionType];

export type TranscodeInput = {
  sourceUrl: string;
  containerFormat?: ContainerFormat;
  width?: number;
  destinationUrl?: string | null;
};

export type TranscodeOutput = {
  /**
   * Gets the id of the blob that contains the media.
   */
  id: string;
  /**
   * Gets a value indicating whether the media is available.
   */
  available: boolean;
  /**
   * Gets a url that can be used to preview the media.
   */
  url?: string | null;
  /**
   * Get when the url is set to expire
   */
  urlExpiresAt?: string | null;
  /**
   * Get the id of the job that is associated with this media.
   */
  jobId: string;
};

/**
 * Transcoding
 */
export type TranscodeStep = WorkflowStep & {
  $type: 'transcode';
} & {
  input: TranscodeInput;
  output?: TranscodeOutput;
} & {
  $type: 'transcode';
};

export type TryOnUInput = {
  subjectUrl: string;
  garmentUrl: string;
  subjectMaskUrl?: string;
  subjectMaskBlobKey?: string;
  garmentDescription?: string;
  maskSubject?: boolean;
  cropSubject?: boolean;
  steps?: number;
  seed?: number;
};

export type TryOnUOutput = {
  blob: Blob;
};

/**
 * An request for updating a workflow.
 */
export type UpdateWorkflowRequest = {
  status?: UpdateWorkflowStatus;
  /**
   * An optional set of new properties to set on the workflow.
   */
  metadata?: {
    [key: string]: unknown;
  } | null;
  /**
   * An optional set of new tags to set on the workflow.
   */
  tags?: Array<string> | null;
  /**
   * Set to true to remove the mature content restriction on the workflow.
   */
  allowMatureContent?: boolean | null;
};

/**
 * Available statuses for updating workflows.
 */
export const UpdateWorkflowStatus = {
  CANCELED: 'canceled',
} as const;

/**
 * Available statuses for updating workflows.
 */
export type UpdateWorkflowStatus = (typeof UpdateWorkflowStatus)[keyof typeof UpdateWorkflowStatus];

export type UpdateWorkflowStepRequest = {
  /**
   * An set of new properties to set on the workflow step.
   */
  metadata: {
    [key: string]: unknown;
  };
};

export type ValidationProblemDetails = {
  type?: string | null;
  title?: string | null;
  status?: number | null;
  detail?: string | null;
  instance?: string | null;
  errors?: {
    [key: string]: Array<string>;
  };
  [key: string]:
    | unknown
    | (string | null)
    | (string | null)
    | (number | null)
    | (string | null)
    | (string | null)
    | {
        [key: string]: Array<string>;
      }
    | undefined;
};

export type ValueTupleOfStringAndInt32 = {
  [key: string]: never;
};

export const Veo3AspectRatio = {
  '16:9': '16:9',
  '9:16': '9:16',
  '1:1': '1:1',
} as const;

export type Veo3AspectRatio = (typeof Veo3AspectRatio)[keyof typeof Veo3AspectRatio];

export const Veo3Version = {
  '3_0': '3.0',
  '3_1': '3.1',
} as const;

export type Veo3Version = (typeof Veo3Version)[keyof typeof Veo3Version];

export type Veo3VideoGenInput = VideoGenInput & {
  engine: 'veo3';
} & {
  negativePrompt?: string | null;
  enablePromptEnhancer?: boolean;
  aspectRatio?: Veo3AspectRatio;
  duration?: number;
  generateAudio?: boolean;
  seed?: number | null;
  fastMode?: boolean;
  images?: Array<string>;
  version?: Veo3Version;
} & {
  engine: 'veo3';
};

export type VideoBlob = Blob & {
  type: 'video';
} & {
  width?: number | null;
  height?: number | null;
} & {
  type: 'video';
};

export type VideoEnhancementInput = {
  sourceUrl: string;
  upscaler?: VideoEnhancementInputUpscalerOptions;
  interpolation?: VideoEnhancementInputInterpolationOptions;
};

export type VideoEnhancementInputInterpolationOptions = {
  multiplier: number;
};

export type VideoEnhancementInputUpscalerOptions = {
  model?: string | null;
  width: number;
  height: number;
};

export type VideoEnhancementOutput = {
  video: VideoBlob;
};

/**
 * Upscale videos and/or interpolate frames
 */
export type VideoEnhancementStep = WorkflowStep & {
  $type: 'videoEnhancement';
} & {
  input: VideoEnhancementInput;
  output?: VideoEnhancementOutput;
} & {
  $type: 'videoEnhancement';
};

/**
 * Upscale videos and/or interpolate frames
 */
export type VideoEnhancementStepTemplate = WorkflowStepTemplate & {
  $type: 'videoEnhancement';
} & {
  input: VideoEnhancementInput;
} & {
  $type: 'videoEnhancement';
};

/**
 * Represents the input information needed for the VideoFrameExtraction workflow step.
 */
export type VideoFrameExtractionInput = {
  videoUrl: string;
  frameRate?: number;
  uniqueThreshold?: number;
  maxFrames?: number;
};

/**
 * Represents the output from the VideoFrameExtraction workflow step.
 */
export type VideoFrameExtractionOutput = {
  /**
   * A collection of extracted unique video frames as image blobs.
   */
  frames: Array<ImageBlob>;
  /**
   * The total number of unique frames extracted from the video.
   */
  totalFramesExtracted?: number;
};

/**
 * Video Frame Extraction
 */
export type VideoFrameExtractionStep = WorkflowStep & {
  $type: 'videoFrameExtraction';
} & {
  input: VideoFrameExtractionInput;
  output?: VideoFrameExtractionOutput;
} & {
  $type: 'videoFrameExtraction';
};

/**
 * Video Frame Extraction
 */
export type VideoFrameExtractionStepTemplate = WorkflowStepTemplate & {
  $type: 'videoFrameExtraction';
} & {
  input: VideoFrameExtractionInput;
} & {
  $type: 'videoFrameExtraction';
};

export type VideoGenInput = {
  engine: string;
  prompt: string;
};

export type VideoGenInputLora = {
  air: string;
  strength: number;
};

export type VideoGenOutput = {
  video?: VideoBlob;
};

/**
 * Video generation
 */
export type VideoGenStep = WorkflowStep & {
  $type: 'videoGen';
} & {
  input: VideoGenInput;
  output?: VideoGenOutput;
} & {
  $type: 'videoGen';
};

/**
 * Video generation
 */
export type VideoGenStepTemplate = WorkflowStepTemplate & {
  $type: 'videoGen';
} & {
  input: VideoGenInput;
} & {
  $type: 'videoGen';
};

export type VideoInterpolationInput = {
  video: string;
  interpolationFactor?: number;
  model?: string;
};

export type VideoInterpolationOutput = {
  video: VideoBlob;
};

/**
 * Interpolate videos using VFI Mamba
 */
export type VideoInterpolationStep = WorkflowStep & {
  $type: 'videoInterpolation';
} & {
  input: VideoInterpolationInput;
  output?: VideoInterpolationOutput;
} & {
  $type: 'videoInterpolation';
};

/**
 * Interpolate videos using VFI Mamba
 */
export type VideoInterpolationStepTemplate = WorkflowStepTemplate & {
  $type: 'videoInterpolation';
} & {
  input: VideoInterpolationInput;
} & {
  $type: 'videoInterpolation';
};

/**
 * Represents the input information needed for the VideoMetadata workflow step.
 */
export type VideoMetadataInput = {
  /**
   * The video file to extract metadata from.
   */
  video: string;
};

/**
 * Represents the output information returned from the VideoMetadata workflow step.
 */
export type VideoMetadataOutput = {
  /**
   * The width of the video in pixels.
   */
  width: number;
  /**
   * The height of the video in pixels.
   */
  height: number;
  /**
   * The frame rate of the video in frames per second.
   */
  fps: number;
  /**
   * The duration of the video.
   */
  duration: string;
};

/**
 * Extract metadata from videos including width, height, FPS, and duration
 */
export type VideoMetadataStep = WorkflowStep & {
  $type: 'videoMetadata';
} & {
  input: VideoMetadataInput;
  output?: VideoMetadataOutput;
} & {
  $type: 'videoMetadata';
};

/**
 * Extract metadata from videos including width, height, FPS, and duration
 */
export type VideoMetadataStepTemplate = WorkflowStepTemplate & {
  $type: 'videoMetadata';
} & {
  input: VideoMetadataInput;
} & {
  $type: 'videoMetadata';
};

export type VideoUpscalerInput = {
  video: string;
  scaleFactor?: number;
};

export type VideoUpscalerOutput = {
  video: VideoBlob;
};

/**
 * Upscale videos using FlashVSR
 */
export type VideoUpscalerStep = WorkflowStep & {
  $type: 'videoUpscaler';
} & {
  input: VideoUpscalerInput;
  output?: VideoUpscalerOutput;
} & {
  $type: 'videoUpscaler';
};

/**
 * Upscale videos using FlashVSR
 */
export type VideoUpscalerStepTemplate = WorkflowStepTemplate & {
  $type: 'videoUpscaler';
} & {
  input: VideoUpscalerInput;
} & {
  $type: 'videoUpscaler';
};

export type ViduVideoGenInput = VideoGenInput & {
  engine: 'vidu';
} & {
  enablePromptEnhancer?: boolean;
  seed?: number | null;
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  sourceImage?: string | null;
  style?: ViduVideoGenStyle;
  duration?: 4 | 8;
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  endSourceImage?: string | null;
  model?: ViduVideoGenModel;
  aspectRatio?: '16:9' | '9:16' | '1:1';
  movementAmplitude?: 'auto' | 'small' | 'medium' | 'large';
  images?: Array<string>;
  enableBackgroundMusic?: boolean;
} & {
  engine: 'vidu';
};

export const ViduVideoGenModel = {
  DEFAULT: 'default',
  Q1: 'q1',
} as const;

export type ViduVideoGenModel = (typeof ViduVideoGenModel)[keyof typeof ViduVideoGenModel];

export const ViduVideoGenStyle = {
  GENERAL: 'general',
  ANIME: 'anime',
} as const;

export type ViduVideoGenStyle = (typeof ViduVideoGenStyle)[keyof typeof ViduVideoGenStyle];

/**
 * Represents the input information needed for the WDTagging workflow step.
 */
export type WdTaggingInput = {
  /**
   * The model to use for tagging (e.g., "wd14-vit.v1").
   */
  model: string;
  mediaUrl: string;
  /**
   * Optional threshold for tag confidence filtering. Tags below this threshold will be excluded.
   */
  threshold?: number | null;
  /**
   * Optional prompt to guide the tagging process.
   */
  prompt?: string | null;
};

/**
 * Represents the output information returned from the WDTagging workflow step.
 */
export type WdTaggingOutput = {
  /**
   * The detected tags with their confidence scores.
   */
  tags: {
    [key: string]: number;
  };
  /**
   * The content rating scores (general, sensitive, questionable, explicit).
   */
  rating?: {
    [key: string]: number;
  } | null;
};

/**
 * WDTagging
 */
export type WdTaggingStep = WorkflowStep & {
  $type: 'wdTagging';
} & {
  input: WdTaggingInput;
  output?: WdTaggingOutput;
} & {
  $type: 'wdTagging';
};

/**
 * WDTagging
 */
export type WdTaggingStepTemplate = WorkflowStepTemplate & {
  $type: 'wdTagging';
} & {
  input: WdTaggingInput;
} & {
  $type: 'wdTagging';
};

export type Wan21CivitaiVideoGenInput = Wan21VideoGenInput & {
  width?: number;
  height?: number;
  model?: string | null;
  images?: Array<string>;
} & {
  provider: 'civitai';
};

export type Wan21FalVideoGenInput = Wan21VideoGenInput & {
  aspectRatio?: '1:1' | '16:9' | '9:16';
  enablePromptExpansion?: boolean;
} & {
  provider: 'fal';
};

export type Wan21VideoGenInput = WanVideoGenInput & {
  provider: string | null;
} & {
  version: 'v2.1';
};

export type Wan225bFalImageGenInput = Wan225bImageGenInput & {} & {
  provider: 'fal';
};

export type Wan225bFalImageToVideoInput = Wan225bFalVideoGenInput & {
  images?: Array<string>;
} & {
  operation: 'image-to-video';
};

export type Wan225bFalTextToVideoInput = Wan225bFalVideoGenInput & {} & {
  operation: 'text-to-video';
};

export type Wan225bFalVideoGenInput = Wan225bVideoGenInput & {
  operation: string | null;
  resolution?: '480p' | '580p' | '720p';
  aspectRatio?: '1:1' | '16:9' | '9:16' | 'auto';
  enablePromptExpansion?: boolean;
  useDistill?: boolean;
  useFastWan?: boolean;
  interpolatorModel?: 'none' | 'film' | 'rife';
  negativePrompt?: string | null;
  enableSafetyChecker?: boolean;
  numInferenceSteps?: number;
} & {
  provider: 'fal';
};

export type Wan225bImageGenInput = WanImageGenInput & {
  provider: string | null;
  steps?: number;
  shift?: number;
} & {
  version: 'v2.2-5b';
};

export type Wan225bVideoGenInput = WanVideoGenInput & {
  provider: string | null;
} & {
  version: 'v2.2-5b';
};

export type Wan22FalImageGenInput = Wan22ImageGenInput & {
  acceleration?: 'none' | 'fast' | 'faster';
} & {
  provider: 'fal';
};

export type Wan22FalImageToVideoInput = Wan22FalVideoGenInput & {
  images?: Array<string>;
} & {
  operation: 'image-to-video';
};

export type Wan22FalTextToVideoInput = Wan22FalVideoGenInput & {} & {
  operation: 'text-to-video';
};

export type Wan22FalVideoGenInput = Wan22VideoGenInput & {
  operation: string | null;
  resolution?: '480p' | '720p';
  aspectRatio?: '1:1' | '16:9' | '9:16' | '4:3' | '3:4' | '4:5' | '5:4';
  enablePromptExpansion?: boolean;
  shift?: number;
  interpolatorModel?: 'film' | 'rife';
  useTurbo?: boolean;
  negativePrompt?: string | null;
  enableSafetyChecker?: boolean;
} & {
  provider: 'fal';
};

export type Wan22ImageGenInput = WanImageGenInput & {
  provider: string | null;
  steps?: number;
} & {
  version: 'v2.2';
};

export type Wan22VideoGenInput = WanVideoGenInput & {
  provider: string | null;
} & {
  version: 'v2.2';
};

export type Wan25FalImageGenInput = Wan25ImageGenInput & {
  operation: string | null;
} & {
  provider: 'fal';
};

export type Wan25FalImageToImageInput = Wan25FalImageGenInput & {
  images?: Array<string>;
} & {
  operation: 'image-to-image';
};

export type Wan25FalImageToVideoInput = Wan25FalVideoGenInput & {
  images?: Array<string>;
} & {
  operation: 'image-to-video';
};

export type Wan25FalTextToImageInput = Wan25FalImageGenInput & {} & {
  operation: 'text-to-image';
};

export type Wan25FalTextToVideoInput = Wan25FalVideoGenInput & {} & {
  operation: 'text-to-video';
};

export type Wan25FalVideoGenInput = Wan25VideoGenInput & {
  operation: string | null;
  resolution?: '480p' | '720p' | '1080p';
  aspectRatio?: '16:9' | '9:16' | '1:1';
  enablePromptExpansion?: boolean;
  negativePrompt?: string | null;
} & {
  provider: 'fal';
};

export type Wan25ImageGenInput = WanImageGenInput & {
  provider: string | null;
} & {
  version: 'v2.5';
};

export type Wan25VideoGenInput = WanVideoGenInput & {
  provider: string | null;
} & {
  version: 'v2.5';
};

export type WanImageGenInput = ImageGenInput & {
  engine: 'wan';
} & {
  version: string | null;
  prompt: string;
  negativePrompt?: string | null;
  guidanceScale?: number;
  seed?: number | null;
  quantity?: number;
  imageSize?: string;
  enablePromptExpansion?: boolean;
  enableSafetyChecker?: boolean;
  loras?: Array<ImageGenInputLora>;
} & {
  engine: 'wan';
};

export type WanVideoGenInput = VideoGenInput & {
  engine: 'wan';
} & {
  version: string | null;
  /**
   * Either A URL, A DataURL or a Base64 string
   */
  sourceImage?: string | null;
  cfgScale?: number;
  frameRate?: number;
  duration?: number;
  seed?: number | null;
  steps?: number;
  loras?: Array<VideoGenInputLora>;
} & {
  engine: 'wan';
};

/**
 * Details of a workflow.
 */
export type Workflow = {
  /**
   * The ID for the workflow.
   */
  id?: string | null;
  /**
   * The date / time the workflow was created.
   */
  createdAt?: string;
  transactions?: TransactionSummary;
  /**
   * A collection of user defined metadata for the workflow.
   */
  metadata?: {
    [key: string]: unknown;
  };
  status?: WorkflowStatus;
  /**
   * The date / time the workflow was started. Null if not yet started.
   */
  startedAt?: string | null;
  /**
   * The date / time the workflow was completed. Null if not yet complete.
   */
  completedAt?: string | null;
  /**
   * An optional list of tags for the workflow.
   */
  tags?: Array<string>;
  /**
   * Get an associated collection of arguments
   */
  arguments?: {
    [key: string]: unknown;
  };
  /**
   * The steps for the workflow.
   */
  steps?: Array<WorkflowStep>;
  /**
   * An array of callback details for the workflow.
   */
  callbacks?: Array<WorkflowCallback>;
  tips?: WorkflowTips;
  cost?: WorkflowCost;
  nsfwLevel?: NsfwLevel;
  /**
   * Get or set whether this workflow is experimental
   */
  experimental?: boolean | null;
  /**
   * Gets or sets a value indicating whether mature content is allowed in this workflow.
   */
  allowMatureContent?: boolean | null;
  upgradeMode?: WorkflowUpgradeMode;
  /**
   * An optional set of currencies to use for this workflow.
   */
  currencies?: Array<BuzzClientAccount>;
};

/**
 * Details of a callback setup for a workflow.
 */
export type WorkflowCallback = {
  /**
   * The url for the callback.
   */
  url: string;
  /**
   * An array of event types to send to the callback.
   */
  type: Array<
    | 'workflow:*'
    | 'workflow:unassigned'
    | 'workflow:processing'
    | 'workflow:succeeded'
    | 'workflow:failed'
    | 'workflow:expired'
    | 'workflow:canceled'
    | 'step:*'
    | 'step:unassigned'
    | 'step:processing'
    | 'step:succeeded'
    | 'step:failed'
    | 'step:expired'
    | 'step:canceled'
    | 'job:*'
    | 'job:unassigned'
    | 'job:processing'
    | 'job:succeeded'
    | 'job:failed'
    | 'job:expired'
    | 'job:canceled'
  >;
  /**
   * Whether to include detailed output (step/workflow results) in the callback payload.
   */
  detailed?: boolean | null;
};

export type WorkflowCost = {
  /**
   * The base cost of this request, excludsing any tips
   */
  base?: number;
  /**
   * A breakdown of the cost factors for this request
   */
  factors?: {
    [key: string]: number;
  } | null;
  /**
   * A fixed set of cost additions for this request
   */
  fixed?: {
    [key: string]: number;
  } | null;
  tips?: WorkflowCostTips;
  /**
   * The total cost of this request, including tips
   */
  total?: number;
};

/**
 * Get the cost of tips
 */
export type WorkflowCostTips = {
  /**
   * The buzz tipped to Civitai
   */
  civitai: number;
  /**
   * The buzz tipped to the Creators who's resources were used
   */
  creators: number;
};

/**
 * Details of a workflow event.
 */
export type WorkflowEvent = {
  /**
   * The ID that represents the corresponding workflow.
   */
  workflowId: string;
  status: WorkflowStatus;
  /**
   * A timestamp for when this event got raised
   */
  timestamp?: string;
  $type?: string;
  details?: WorkflowEventDetails;
};

/**
 * Detailed information about a workflow included in webhook callbacks when detailed mode is enabled.
 */
export type WorkflowEventDetails = {
  /**
   * Custom metadata associated with the workflow
   */
  metadata?: {
    [key: string]: unknown;
  } | null;
  /**
   * Arguments provided to the workflow
   */
  arguments?: {
    [key: string]: unknown;
  } | null;
  /**
   * When the workflow was created
   */
  createdAt?: string;
  /**
   * When the workflow completed execution
   */
  completedAt?: string | null;
  /**
   * When the workflow started execution
   */
  startedAt?: string | null;
  /**
   * Details about each step in the workflow
   */
  steps?: Array<WorkflowEventStepDetails>;
};

/**
 * Detailed information about a workflow step within a workflow event.
 */
export type WorkflowEventStepDetails = {
  /**
   * The name of the step
   */
  name: string;
  status: WorkflowStatus;
  /**
   * When the step started execution
   */
  startedAt?: string | null;
  /**
   * When the step completed execution
   */
  completedAt?: string | null;
  /**
   * Custom metadata associated with the step
   */
  metadata?: {
    [key: string]: unknown;
  } | null;
  /**
   * The input configuration for the step
   */
  input?: unknown;
  /**
   * The output result from the step
   */
  output?: unknown;
  /**
   * The cost of executing the step
   */
  cost?: number | null;
};

/**
 * Values available to represent workflow status.
 */
export const WorkflowStatus = {
  UNASSIGNED: 'unassigned',
  PREPARING: 'preparing',
  SCHEDULED: 'scheduled',
  PROCESSING: 'processing',
  SUCCEEDED: 'succeeded',
  FAILED: 'failed',
  EXPIRED: 'expired',
  CANCELED: 'canceled',
} as const;

/**
 * Values available to represent workflow status.
 */
export type WorkflowStatus = (typeof WorkflowStatus)[keyof typeof WorkflowStatus];

/**
 * Details of a workflow step.
 */
export type WorkflowStep = {
  $type: string;
  /**
   * The name of the workflow step. Used to allow steps to refer to one another.
   */
  name: string;
  priority?: Priority;
  /**
   * The maximum time to wait for this step to complete.
   */
  timeout?: string | null;
  /**
   * The maximum number of times this step should be retried.
   */
  retries?: number | null;
  /**
   * The jobs generated by this step.
   */
  jobs?: Array<WorkflowStepJob> | null;
  status?: WorkflowStatus;
  /**
   * The date / time the step was started. Null if not yet started.
   */
  startedAt?: string | null;
  /**
   * The date / time the step was completed. Null if not yet completed.
   */
  completedAt?: string | null;
  /**
   * A collection of user defined metadata for the workflow step.
   */
  metadata?: {
    [key: string]: unknown;
  };
  /**
   * An estimation on the current progression of this step, or null if there is no estimation
   */
  estimatedProgressRate?: number | null;
};

/**
 * Details of a workflow step event.
 */
export type WorkflowStepEvent = {
  /**
   * The workflow ID.
   */
  workflowId: string;
  /**
   * The workflow step's name.
   */
  name: string;
  status: WorkflowStatus;
  $type?: string;
  details?: WorkflowStepEventDetails;
};

/**
 * Detailed information about a workflow step included in webhook callbacks when detailed mode is enabled.
 */
export type WorkflowStepEventDetails = {
  /**
   * When the step started execution
   */
  startedAt?: string | null;
  /**
   * When the step completed execution
   */
  completedAt?: string | null;
  /**
   * Estimated progress rate of the step (0.0 to 1.0)
   */
  estimatedProgressRate?: number | null;
  /**
   * Number of times the step has been retried
   */
  retries?: number;
  /**
   * Custom metadata associated with the step
   */
  metadata?: {
    [key: string]: unknown;
  } | null;
  /**
   * The input configuration for the step
   */
  input?: unknown;
  /**
   * The output result from the step
   */
  output?: unknown;
};

/**
 * Details of a job produced by a workflow step.
 */
export type WorkflowStepJob = {
  /**
   * The job's ID.
   */
  id: string;
  status?: WorkflowStatus;
  /**
   * The date / time the job started. Null if not yet started.
   */
  startedAt?: string | null;
  /**
   * The date / time the job completed. Null if not yet completed.
   */
  completedAt?: string | null;
  queuePosition?: WorkflowStepJobQueuePosition;
  /**
   * The job's cost.
   */
  cost?: number;
  /**
   * An estimation on the current progression of this job, or null if there is no estimation
   */
  estimatedProgressRate?: number | null;
};

/**
 * Details of a workflow step job event.
 */
export type WorkflowStepJobEvent = {
  /**
   * The workflow ID.
   */
  workflowId: string;
  /**
   * The step's name.
   */
  stepName: string;
  /**
   * The job's ID.
   */
  jobId: string;
  status: WorkflowStatus;
  $type?: string;
  progress?: number | null;
  reason?: string | null;
  blockedReason?: string | null;
  matureContent?: boolean | null;
};

/**
 * Details of the workflow step job's queue position.
 */
export type WorkflowStepJobQueuePosition = {
  support: JobSupport;
  /**
   * The number of preceding jobs in the queue.
   */
  precedingJobs?: number | null;
  /**
   * An estimated date / time for when the job will start.
   */
  startAt?: string | null;
  /**
   * An estimated date / time for when the job will complete.
   */
  completeAt?: string | null;
};

/**
 * Details of a workflow step template.
 */
export type WorkflowStepTemplate = {
  $type: string;
  /**
   * The name of the workflow step. Used to allow steps to refer to one another.
   */
  name?: string | null;
  priority?: Priority;
  /**
   * The maximum time to wait for this step to complete.
   */
  timeout?: string | null;
  /**
   * The maximum number of times this step should be retried.
   */
  retries?: number | null;
  /**
   * A collection of user defined metadata for the workflow step.
   */
  metadata?: {
    [key: string]: unknown;
  } | null;
};

/**
 * Details of a requested workflow.
 */
export type WorkflowTemplate = {
  /**
   * A collection of user defined metadata that can be used to store additional information about the workflow.
   */
  metadata?: {
    [key: string]: unknown;
  } | null;
  /**
   * A list of tags associated with this workflow.
   * Tags are indexed and can be used to search for workflows.
   * At most 10 tags can be assigned to a workflow. Each tag can be at most 200 characters long.
   */
  tags?: Array<string> | null;
  /**
   * An array of steps that compose this workflow.
   */
  steps: Array<WorkflowStepTemplate>;
  /**
   * An array of callbacks to be triggered during the lifetime of the workflow.
   */
  callbacks?: Array<WorkflowCallback> | null;
  tips?: WorkflowTips;
  /**
   * Get an associated collection of arguments
   */
  arguments?: {
    [key: string]: unknown;
  } | null;
  nsfwLevel?: NsfwLevel;
  /**
   * Get or set whether this workflow is experimental
   */
  experimental?: boolean | null;
  /**
   * Get or set whether this workflow should allow mature content.
   * When set to false, the workflow will not return any content that is marked as mature.
   * Additional payment options are available for workflows that do not allow mature content.
   */
  allowMatureContent?: boolean | null;
  upgradeMode?: WorkflowUpgradeMode;
  /**
   * Limit the currencies that can be used to pay for this workflow.
   */
  currencies?: Array<BuzzClientAccount>;
};

export type WorkflowTips = {
  /**
   * The rate of tipping that should be allocated to civitai
   */
  civitai: number;
  /**
   * The rate of tipping that should be allocated to creators involved in this workflow
   */
  creators: number;
};

/**
 * Specifies how a workflow should be upgraded when mature content is detected and green or blue buzz was used for payment.
 */
export const WorkflowUpgradeMode = {
  MANUAL: 'manual',
  AUTOMATIC: 'automatic',
} as const;

/**
 * Specifies how a workflow should be upgraded when mature content is detected and green or blue buzz was used for payment.
 */
export type WorkflowUpgradeMode = (typeof WorkflowUpgradeMode)[keyof typeof WorkflowUpgradeMode];

export type ZImageImageGenInput = SdCppImageGenInput & {
  model: string;
} & {
  ecosystem: 'zImage';
};

/**
 * AI Toolkit training for Z Image Turbo models
 */
export type ZImageTurboAiToolkitTrainingInput = AiToolkitTrainingInput & {} & {
  ecosystem: 'zimageturbo';
};

export type ZImageTurboCreateImageGenInput = ZImageTurboImageGenInput & {
  width?: number;
  height?: number;
} & {
  operation: 'createImage';
};

export type ZImageTurboImageGenInput = ZImageImageGenInput & {
  operation: string;
  prompt: string;
  negativePrompt?: string | null;
  sampleMethod?: SdCppSampleMethod;
  schedule?: SdCppSchedule;
  steps?: number;
  cfgScale?: number;
  seed?: number | null;
  quantity?: number;
} & {
  model: 'turbo';
};

/**
 * Training data packaged as a zip file
 */
export type ZipTrainingData = TrainingData & {
  type: 'zip';
} & {
  /**
   * AIR pointing to the zip file containing training data
   */
  sourceUrl: string;
  /**
   * The number of images/frames/items in this training data
   */
  count: number;
} & {
  type: 'zip';
};

export const ZoeDepthEnvironment = {
  INDOOR: 'indoor',
  OUTDOOR: 'outdoor',
} as const;

export type ZoeDepthEnvironment = (typeof ZoeDepthEnvironment)[keyof typeof ZoeDepthEnvironment];

export type GetBlobData = {
  body?: never;
  path: {
    /**
     * The blob ID to retrieve.
     */
    blobId: string;
  };
  query?: {
    /**
     * The id of the workflow to obtain
     */
    workflowId?: string;
    /**
     * A maximum nsfw level. If this is specified and the blob does not have a NSFW level specified or the NSFW level exceeds our max then we'll return an error
     */
    nsfwLevel?: NsfwLevel;
  };
  url: '/v2/consumer/blobs/{blobId}';
};

export type GetBlobErrors = {
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Forbidden
   */
  403: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type GetBlobError = GetBlobErrors[keyof GetBlobErrors];

export type HeadBlobData = {
  body?: never;
  path: {
    /**
     * Identifies the specific blob to check for existence and NSFW level.
     */
    blobId: string;
  };
  query?: never;
  url: '/v2/consumer/blobs/{blobId}';
};

export type HeadBlobErrors = {
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type HeadBlobError = HeadBlobErrors[keyof HeadBlobErrors];

export type HeadBlobResponses = {
  /**
   * No Content
   */
  204: void;
};

export type HeadBlobResponse = HeadBlobResponses[keyof HeadBlobResponses];

export type GetBlobContentData = {
  body?: never;
  path: {
    /**
     * The encrypted token containing blob access parameters
     */
    encryptedToken: string;
  };
  query?: never;
  url: '/v2/consumer/blobs/content/{encryptedToken}';
};

export type GetBlobContentErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
  /**
   * Internal Server Error
   */
  500: unknown;
};

export type GetBlobContentError = GetBlobContentErrors[keyof GetBlobContentErrors];

export type GetBlobContentResponses = {
  /**
   * OK
   */
  200: unknown;
};

export type InvokeAgeClassificationStepTemplateData = {
  body?: AgeClassificationInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/ageClassification';
};

export type InvokeAgeClassificationStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeAgeClassificationStepTemplateError =
  InvokeAgeClassificationStepTemplateErrors[keyof InvokeAgeClassificationStepTemplateErrors];

export type InvokeAgeClassificationStepTemplateResponses = {
  /**
   * OK
   */
  200: AgeClassificationOutput;
};

export type InvokeAgeClassificationStepTemplateResponse =
  InvokeAgeClassificationStepTemplateResponses[keyof InvokeAgeClassificationStepTemplateResponses];

export type InvokeComfyStepTemplateData = {
  body?: ComfyInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/comfy';
};

export type InvokeComfyStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeComfyStepTemplateError =
  InvokeComfyStepTemplateErrors[keyof InvokeComfyStepTemplateErrors];

export type InvokeComfyStepTemplateResponses = {
  /**
   * OK
   */
  200: ComfyOutput;
};

export type InvokeComfyStepTemplateResponse =
  InvokeComfyStepTemplateResponses[keyof InvokeComfyStepTemplateResponses];

export type InvokeEchoStepTemplateData = {
  body?: EchoInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/echo';
};

export type InvokeEchoStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeEchoStepTemplateError =
  InvokeEchoStepTemplateErrors[keyof InvokeEchoStepTemplateErrors];

export type InvokeEchoStepTemplateResponses = {
  /**
   * OK
   */
  200: EchoOutput;
};

export type InvokeEchoStepTemplateResponse =
  InvokeEchoStepTemplateResponses[keyof InvokeEchoStepTemplateResponses];

export type InvokeImageGenStepTemplateData = {
  body?: ImageGenInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/imageGen';
};

export type InvokeImageGenStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeImageGenStepTemplateError =
  InvokeImageGenStepTemplateErrors[keyof InvokeImageGenStepTemplateErrors];

export type InvokeImageGenStepTemplateResponses = {
  /**
   * OK
   */
  200: ImageGenOutput;
};

export type InvokeImageGenStepTemplateResponse =
  InvokeImageGenStepTemplateResponses[keyof InvokeImageGenStepTemplateResponses];

export type InvokeImageResourceTrainingStepTemplateData = {
  body?: ImageResourceTrainingInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/imageResourceTraining';
};

export type InvokeImageResourceTrainingStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeImageResourceTrainingStepTemplateError =
  InvokeImageResourceTrainingStepTemplateErrors[keyof InvokeImageResourceTrainingStepTemplateErrors];

export type InvokeImageResourceTrainingStepTemplateResponses = {
  /**
   * OK
   */
  200: ImageResourceTrainingOutput;
};

export type InvokeImageResourceTrainingStepTemplateResponse =
  InvokeImageResourceTrainingStepTemplateResponses[keyof InvokeImageResourceTrainingStepTemplateResponses];

export type InvokeImageUploadStepTemplateData = {
  body?: string;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/imageUpload';
};

export type InvokeImageUploadStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeImageUploadStepTemplateError =
  InvokeImageUploadStepTemplateErrors[keyof InvokeImageUploadStepTemplateErrors];

export type InvokeImageUploadStepTemplateResponses = {
  /**
   * OK
   */
  200: ImageUploadOutput;
};

export type InvokeImageUploadStepTemplateResponse =
  InvokeImageUploadStepTemplateResponses[keyof InvokeImageUploadStepTemplateResponses];

export type InvokeImageUpscalerStepTemplateData = {
  body?: ImageUpscalerInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/imageUpscaler';
};

export type InvokeImageUpscalerStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeImageUpscalerStepTemplateError =
  InvokeImageUpscalerStepTemplateErrors[keyof InvokeImageUpscalerStepTemplateErrors];

export type InvokeImageUpscalerStepTemplateResponses = {
  /**
   * OK
   */
  200: ImageUpscalerOutput;
};

export type InvokeImageUpscalerStepTemplateResponse =
  InvokeImageUpscalerStepTemplateResponses[keyof InvokeImageUpscalerStepTemplateResponses];

export type InvokeMediaHashStepTemplateData = {
  body?: MediaHashInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/mediaHash';
};

export type InvokeMediaHashStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeMediaHashStepTemplateError =
  InvokeMediaHashStepTemplateErrors[keyof InvokeMediaHashStepTemplateErrors];

export type InvokeMediaHashStepTemplateResponses = {
  /**
   * OK
   */
  200: MediaHashOutput;
};

export type InvokeMediaHashStepTemplateResponse =
  InvokeMediaHashStepTemplateResponses[keyof InvokeMediaHashStepTemplateResponses];

export type InvokeMediaRatingStepTemplateData = {
  body?: MediaRatingInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/mediaRating';
};

export type InvokeMediaRatingStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeMediaRatingStepTemplateError =
  InvokeMediaRatingStepTemplateErrors[keyof InvokeMediaRatingStepTemplateErrors];

export type InvokeMediaRatingStepTemplateResponses = {
  /**
   * OK
   */
  200: MediaRatingOutput;
};

export type InvokeMediaRatingStepTemplateResponse =
  InvokeMediaRatingStepTemplateResponses[keyof InvokeMediaRatingStepTemplateResponses];

export type InvokePreprocessImageStepTemplateData = {
  body?: PreprocessImageInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/preprocessImage';
};

export type InvokePreprocessImageStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokePreprocessImageStepTemplateError =
  InvokePreprocessImageStepTemplateErrors[keyof InvokePreprocessImageStepTemplateErrors];

export type InvokePreprocessImageStepTemplateResponses = {
  /**
   * OK
   */
  200: PreprocessImageOutput;
};

export type InvokePreprocessImageStepTemplateResponse =
  InvokePreprocessImageStepTemplateResponses[keyof InvokePreprocessImageStepTemplateResponses];

export type InvokeTextToImageStepTemplateData = {
  body?: TextToImageInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/textToImage';
};

export type InvokeTextToImageStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeTextToImageStepTemplateError =
  InvokeTextToImageStepTemplateErrors[keyof InvokeTextToImageStepTemplateErrors];

export type InvokeTextToImageStepTemplateResponses = {
  /**
   * OK
   */
  200: TextToImageOutput;
};

export type InvokeTextToImageStepTemplateResponse =
  InvokeTextToImageStepTemplateResponses[keyof InvokeTextToImageStepTemplateResponses];

export type InvokeTrainingStepTemplateData = {
  body?: TrainingInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/training';
};

export type InvokeTrainingStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeTrainingStepTemplateError =
  InvokeTrainingStepTemplateErrors[keyof InvokeTrainingStepTemplateErrors];

export type InvokeTrainingStepTemplateResponses = {
  /**
   * OK
   */
  200: TrainingOutput;
};

export type InvokeTrainingStepTemplateResponse =
  InvokeTrainingStepTemplateResponses[keyof InvokeTrainingStepTemplateResponses];

export type InvokeVideoEnhancementStepTemplateData = {
  body?: VideoEnhancementInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/videoEnhancement';
};

export type InvokeVideoEnhancementStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeVideoEnhancementStepTemplateError =
  InvokeVideoEnhancementStepTemplateErrors[keyof InvokeVideoEnhancementStepTemplateErrors];

export type InvokeVideoEnhancementStepTemplateResponses = {
  /**
   * OK
   */
  200: VideoEnhancementOutput;
};

export type InvokeVideoEnhancementStepTemplateResponse =
  InvokeVideoEnhancementStepTemplateResponses[keyof InvokeVideoEnhancementStepTemplateResponses];

export type InvokeVideoFrameExtractionStepTemplateData = {
  body?: VideoFrameExtractionInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/videoFrameExtraction';
};

export type InvokeVideoFrameExtractionStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeVideoFrameExtractionStepTemplateError =
  InvokeVideoFrameExtractionStepTemplateErrors[keyof InvokeVideoFrameExtractionStepTemplateErrors];

export type InvokeVideoFrameExtractionStepTemplateResponses = {
  /**
   * OK
   */
  200: VideoFrameExtractionOutput;
};

export type InvokeVideoFrameExtractionStepTemplateResponse =
  InvokeVideoFrameExtractionStepTemplateResponses[keyof InvokeVideoFrameExtractionStepTemplateResponses];

export type InvokeVideoGenStepTemplateData = {
  body?: VideoGenInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/videoGen';
};

export type InvokeVideoGenStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeVideoGenStepTemplateError =
  InvokeVideoGenStepTemplateErrors[keyof InvokeVideoGenStepTemplateErrors];

export type InvokeVideoGenStepTemplateResponses = {
  /**
   * OK
   */
  200: VideoGenOutput;
};

export type InvokeVideoGenStepTemplateResponse =
  InvokeVideoGenStepTemplateResponses[keyof InvokeVideoGenStepTemplateResponses];

export type InvokeVideoInterpolationStepTemplateData = {
  body?: VideoInterpolationInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/videoInterpolation';
};

export type InvokeVideoInterpolationStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeVideoInterpolationStepTemplateError =
  InvokeVideoInterpolationStepTemplateErrors[keyof InvokeVideoInterpolationStepTemplateErrors];

export type InvokeVideoInterpolationStepTemplateResponses = {
  /**
   * OK
   */
  200: VideoInterpolationOutput;
};

export type InvokeVideoInterpolationStepTemplateResponse =
  InvokeVideoInterpolationStepTemplateResponses[keyof InvokeVideoInterpolationStepTemplateResponses];

export type InvokeVideoMetadataStepTemplateData = {
  body?: VideoMetadataInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/videoMetadata';
};

export type InvokeVideoMetadataStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeVideoMetadataStepTemplateError =
  InvokeVideoMetadataStepTemplateErrors[keyof InvokeVideoMetadataStepTemplateErrors];

export type InvokeVideoMetadataStepTemplateResponses = {
  /**
   * OK
   */
  200: VideoMetadataOutput;
};

export type InvokeVideoMetadataStepTemplateResponse =
  InvokeVideoMetadataStepTemplateResponses[keyof InvokeVideoMetadataStepTemplateResponses];

export type InvokeVideoUpscalerStepTemplateData = {
  body?: VideoUpscalerInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/videoUpscaler';
};

export type InvokeVideoUpscalerStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeVideoUpscalerStepTemplateError =
  InvokeVideoUpscalerStepTemplateErrors[keyof InvokeVideoUpscalerStepTemplateErrors];

export type InvokeVideoUpscalerStepTemplateResponses = {
  /**
   * OK
   */
  200: VideoUpscalerOutput;
};

export type InvokeVideoUpscalerStepTemplateResponse =
  InvokeVideoUpscalerStepTemplateResponses[keyof InvokeVideoUpscalerStepTemplateResponses];

export type InvokeWdTaggingStepTemplateData = {
  body?: WdTaggingInput;
  path?: never;
  query?: {
    experimental?: boolean;
    allowMatureContent?: boolean;
  };
  url: '/v2/consumer/recipes/wdTagging';
};

export type InvokeWdTaggingStepTemplateErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type InvokeWdTaggingStepTemplateError =
  InvokeWdTaggingStepTemplateErrors[keyof InvokeWdTaggingStepTemplateErrors];

export type InvokeWdTaggingStepTemplateResponses = {
  /**
   * OK
   */
  200: WdTaggingOutput;
};

export type InvokeWdTaggingStepTemplateResponse =
  InvokeWdTaggingStepTemplateResponses[keyof InvokeWdTaggingStepTemplateResponses];

export type InvalidateResourceData = {
  body?: never;
  path: {
    /**
     * A unique ID for the resource being requested. See https://developer.civitai.com/docs/getting-started/ai-resource-identifier for more info on AIRs.
     */
    air: string;
  };
  query?: {
    /**
     * One or more userIds to invalidate early access for
     */
    userId?: Array<number>;
    etag?: string;
  };
  url: '/v2/resources/{air}';
};

export type InvalidateResourceErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
};

export type InvalidateResourceError = InvalidateResourceErrors[keyof InvalidateResourceErrors];

export type InvalidateResourceResponses = {
  /**
   * No Content
   */
  204: void;
};

export type InvalidateResourceResponse =
  InvalidateResourceResponses[keyof InvalidateResourceResponses];

export type GetResourceData = {
  body?: never;
  path: {
    /**
     * A unique ID for the resource being requested. See https://developer.civitai.com/docs/getting-started/ai-resource-identifier for more info on AIRs.
     */
    air: string;
  };
  query?: never;
  url: '/v2/resources/{air}';
};

export type GetResourceErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type GetResourceError = GetResourceErrors[keyof GetResourceErrors];

export type GetResourceResponses = {
  /**
   * OK
   */
  200: ResourceInfo;
};

export type GetResourceResponse = GetResourceResponses[keyof GetResourceResponses];

export type QueryWorkflowsData = {
  body?: never;
  headers?: {
    /**
     * Specify 'application/zip' to get the response as a zip file
     */
    Accept?: string;
  };
  path?: never;
  query?: {
    /**
     * An optional cursor to continue querying workflows from a previous query.
     */
    cursor?: string;
    /**
     * How many workflows to return
     */
    take?: number;
    /**
     * An optional list of tags to query by
     */
    tags?: Array<string>;
    /**
     * An optional additional query that is used to match workflows through metadata
     */
    query?: string;
    /**
     * Whether to return data from oldest to newest
     */
    ascending?: boolean;
    /**
     * When set to true, any blob that has mature won't be available and won't have a URL
     */
    hideMatureContent?: boolean;
  };
  url: '/v2/consumer/workflows';
};

export type QueryWorkflowsErrors = {
  /**
   * Unauthorized
   */
  401: ProblemDetails;
};

export type QueryWorkflowsError = QueryWorkflowsErrors[keyof QueryWorkflowsErrors];

export type QueryWorkflowsResponses = {
  /**
   * OK
   */
  200: CursedArrayOfTelemetryCursorAndWorkflow;
};

export type QueryWorkflowsResponse = QueryWorkflowsResponses[keyof QueryWorkflowsResponses];

export type SubmitWorkflowData = {
  body?: WorkflowTemplate;
  path?: never;
  query?: {
    /**
     * Whether to wait for the workflow to complete before returning or to return immediately
     * The request may return a 202 if the clients waits for the workflow to complete and the workflow does not complete within the requested timeout.
     * In which case the client should use the token to query the status of the workflow.
     */
    wait?: number;
    /**
     * Whether to actually submit the workflow or return an estimate on what would happen upon submission
     */
    whatif?: boolean;
    /**
     * When set to true, any blob that has mature won't be available and won't have a URL
     */
    hideMatureContent?: boolean;
  };
  url: '/v2/consumer/workflows';
};

export type SubmitWorkflowErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Forbidden
   */
  403: string;
  /**
   * Too Many Requests
   */
  429: ProblemDetails;
};

export type SubmitWorkflowError = SubmitWorkflowErrors[keyof SubmitWorkflowErrors];

export type SubmitWorkflowResponses = {
  /**
   * OK
   */
  200: Workflow;
  /**
   * Accepted
   */
  202: Workflow;
};

export type SubmitWorkflowResponse = SubmitWorkflowResponses[keyof SubmitWorkflowResponses];

export type DeleteWorkflowData = {
  body?: never;
  path: {
    /**
     * The ID of the workflow to delete.
     */
    workflowId: string;
  };
  query?: never;
  url: '/v2/consumer/workflows/{workflowId}';
};

export type DeleteWorkflowErrors = {
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type DeleteWorkflowError = DeleteWorkflowErrors[keyof DeleteWorkflowErrors];

export type DeleteWorkflowResponses = {
  /**
   * No Content
   */
  204: void;
};

export type DeleteWorkflowResponse = DeleteWorkflowResponses[keyof DeleteWorkflowResponses];

export type GetWorkflowData = {
  body?: never;
  path: {
    /**
     * The ID of the workflow to get status for
     */
    workflowId: string;
  };
  query?: {
    /**
     * Whether to wait for the workflow to complete before returning or to return immediately
     * The request may return a 202 if the clients waits for the workflow to complete and the workflow does not complete within the requested timeout.
     * In which case the client should use the token to query the status of the workflow.
     */
    wait?: boolean;
    /**
     * When set to true, any blob that has mature won't be available and won't have a URL
     */
    hideMatureContent?: boolean;
  };
  url: '/v2/consumer/workflows/{workflowId}';
};

export type GetWorkflowErrors = {
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type GetWorkflowError = GetWorkflowErrors[keyof GetWorkflowErrors];

export type GetWorkflowResponses = {
  /**
   * OK
   */
  200: Workflow;
};

export type GetWorkflowResponse = GetWorkflowResponses[keyof GetWorkflowResponses];

export type PatchWorkflowData = {
  /**
   * A valid PATCH document
   */
  body?: JsonPatchDocument;
  path: {
    /**
     * The ID of the workflow to patch
     */
    workflowId: string;
  };
  query?: never;
  url: '/v2/consumer/workflows/{workflowId}';
};

export type PatchWorkflowErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type PatchWorkflowError = PatchWorkflowErrors[keyof PatchWorkflowErrors];

export type PatchWorkflowResponses = {
  /**
   * No Content
   */
  204: void;
};

export type PatchWorkflowResponse = PatchWorkflowResponses[keyof PatchWorkflowResponses];

export type UpdateWorkflowData = {
  /**
   * The details to update on the workflow.
   */
  body?: UpdateWorkflowRequest;
  path: {
    /**
     * The ID of the worfklow to update.
     */
    workflowId: string;
  };
  query?: never;
  url: '/v2/consumer/workflows/{workflowId}';
};

export type UpdateWorkflowErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type UpdateWorkflowError = UpdateWorkflowErrors[keyof UpdateWorkflowErrors];

export type UpdateWorkflowResponses = {
  /**
   * No Content
   */
  204: void;
};

export type UpdateWorkflowResponse = UpdateWorkflowResponses[keyof UpdateWorkflowResponses];

export type RemoveAllWorkflowTagsData = {
  body?: never;
  path: {
    /**
     * The ID of the worfklow to update.
     */
    workflowId: string;
  };
  query?: never;
  url: '/v2/consumer/workflows/{workflowId}/tags';
};

export type RemoveAllWorkflowTagsErrors = {
  /**
   * Bad Request
   */
  400: ValidationProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type RemoveAllWorkflowTagsError =
  RemoveAllWorkflowTagsErrors[keyof RemoveAllWorkflowTagsErrors];

export type RemoveAllWorkflowTagsResponses = {
  /**
   * No Content
   */
  204: void;
};

export type RemoveAllWorkflowTagsResponse =
  RemoveAllWorkflowTagsResponses[keyof RemoveAllWorkflowTagsResponses];

export type AddWorkflowTagData = {
  /**
   * The the tag to add to the workflow.
   */
  body?: string;
  path: {
    /**
     * The ID of the worfklow to update.
     */
    workflowId: string;
  };
  query?: never;
  url: '/v2/consumer/workflows/{workflowId}/tags';
};

export type AddWorkflowTagErrors = {
  /**
   * Bad Request
   */
  400: ValidationProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type AddWorkflowTagError = AddWorkflowTagErrors[keyof AddWorkflowTagErrors];

export type AddWorkflowTagResponses = {
  /**
   * No Content
   */
  204: void;
};

export type AddWorkflowTagResponse = AddWorkflowTagResponses[keyof AddWorkflowTagResponses];

export type RemoveWorkflowTagData = {
  body?: never;
  path: {
    /**
     * The ID of the worfklow to update.
     */
    workflowId: string;
    /**
     * The the tag to remove from the workflow.
     */
    tag: string;
  };
  query?: never;
  url: '/v2/consumer/workflows/{workflowId}/tags/{tag}';
};

export type RemoveWorkflowTagErrors = {
  /**
   * Bad Request
   */
  400: ValidationProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type RemoveWorkflowTagError = RemoveWorkflowTagErrors[keyof RemoveWorkflowTagErrors];

export type RemoveWorkflowTagResponses = {
  /**
   * No Content
   */
  204: void;
};

export type RemoveWorkflowTagResponse =
  RemoveWorkflowTagResponses[keyof RemoveWorkflowTagResponses];

export type GetWorkflowStepData = {
  body?: never;
  path: {
    /**
     * The id of the workflow to get status for
     */
    workflowId: string;
    /**
     * The name of the step within the workflow to get status for
     */
    stepName: string;
  };
  query?: never;
  url: '/v2/consumer/workflows/{workflowId}/steps/{stepName}';
};

export type GetWorkflowStepErrors = {
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type GetWorkflowStepError = GetWorkflowStepErrors[keyof GetWorkflowStepErrors];

export type GetWorkflowStepResponses = {
  /**
   * OK
   */
  200: WorkflowStep;
};

export type GetWorkflowStepResponse = GetWorkflowStepResponses[keyof GetWorkflowStepResponses];

export type PatchWorkflowStepData = {
  body?: JsonPatchDocument;
  path: {
    workflowId: string;
    stepName: string;
  };
  query?: never;
  url: '/v2/consumer/workflows/{workflowId}/steps/{stepName}';
};

export type PatchWorkflowStepErrors = {
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type PatchWorkflowStepError = PatchWorkflowStepErrors[keyof PatchWorkflowStepErrors];

export type PatchWorkflowStepResponses = {
  /**
   * No Content
   */
  204: void;
};

export type PatchWorkflowStepResponse =
  PatchWorkflowStepResponses[keyof PatchWorkflowStepResponses];

export type UpdateWorkflowStepData = {
  /**
   * The details to update on the workflow step.
   */
  body?: UpdateWorkflowStepRequest;
  path: {
    /**
     * The id of the workflow to update.
     */
    workflowId: string;
    /**
     * The name of the step to update.
     */
    stepName: string;
  };
  query?: never;
  url: '/v2/consumer/workflows/{workflowId}/steps/{stepName}';
};

export type UpdateWorkflowStepErrors = {
  /**
   * Bad Request
   */
  400: ProblemDetails;
  /**
   * Unauthorized
   */
  401: ProblemDetails;
  /**
   * Not Found
   */
  404: ProblemDetails;
};

export type UpdateWorkflowStepError = UpdateWorkflowStepErrors[keyof UpdateWorkflowStepErrors];

export type UpdateWorkflowStepResponses = {
  /**
   * No Content
   */
  204: void;
};

export type UpdateWorkflowStepResponse =
  UpdateWorkflowStepResponses[keyof UpdateWorkflowStepResponses];

export type ClientOptions = {
  baseUrl: `${string}://swagger.json` | (string & {});
};
